{
  "paragraphs": [
    {
      "text": "%sh\npip install pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2022-04-29 18:42:55.776",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.2.1)\nRequirement already satisfied: py4j\u003d\u003d0.10.9.3 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.3)\nWARNING: Running pip as the \u0027root\u0027 user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1651231689781_491229263",
      "id": "paragraph_1651231689781_491229263",
      "dateCreated": "2022-04-29 11:28:09.781",
      "dateStarted": "2022-04-29 18:42:55.798",
      "dateFinished": "2022-04-29 18:42:58.547",
      "status": "FINISHED"
    },
    {
      "text": "%python\n\nimport pyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType\n\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import StringType\n\nspark_session \u003d SparkSession \\\n    .builder \\\n    .appName(\"kafka-to-spark\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n    \ndf \u003d spark_session \\\n  .read \\\n  .format(\"kafka\") \\\n  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n  .option(\"subscribe\", \"input\") \\\n  .option(\"startingOffsets\", \"earliest\") \\\n  .option(\"maxOffsetsPerTrigger\", 10) \\\n  .load()\n  \ndf.printSchema()\n  \ndf1 \u003d df.selectExpr(\"CAST(value AS STRING)\")\n\nschema \u003d StructType([ \\\nStructField(\"video_unique\",StringType()), \\\nStructField(\"num\",IntegerType()), \\\nStructField(\"chat_time\",TimestampType()), \\\nStructField(\"chat_id\",StringType()), \\\nStructField(\"chat_message\",StringType()), \\\n])\n\ndf2 \u003d df1.select(from_json(\"value\",schema).alias(\"data\")).select(\"data.*\")\n\n#   .outputMode(\"append\") \\\n  \ndf2.show()",
      "user": "anonymous",
      "dateUpdated": "2022-04-30 02:46:08.405",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "22/04/30 02:46:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark\u0027s default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/04/30 02:46:17 WARN KafkaSourceProvider: maxOffsetsPerTrigger option ignored in batch queries\nroot\n |-- key: binary (nullable \u003d true)\n |-- value: binary (nullable \u003d true)\n |-- topic: string (nullable \u003d true)\n |-- partition: integer (nullable \u003d true)\n |-- offset: long (nullable \u003d true)\n |-- timestamp: timestamp (nullable \u003d true)\n |-- timestampType: integer (nullable \u003d true)\n\n\r[Stage 0:\u003e                                                          (0 + 1) / 1]\r+------------+---+-------------------+----------------------+---------------------------------+\n|video_unique|num|          chat_time|               chat_id|                     chat_message|\n+------------+---+-------------------+----------------------+---------------------------------+\n| py_phbQxy5Y|  0|2022-04-29 18:43:40|          monte kristo|                광연 몇 살이 에요|\n| py_phbQxy5Y|  1|2022-04-29 18:43:49|            쇼팬하우어|                         모 냐 고|\n| py_phbQxy5Y|  2|2022-04-29 18:43:50|                  왕19| 철면피 전략 으로 밀다 부치다 ...|\n| py_phbQxy5Y|  3|2022-04-29 18:43:52|                  캔디|    간질 대 모집단 근데 능력 없다|\n| py_phbQxy5Y|  4|2022-04-29 18:44:01|             hasinmajo|       정치 쟁이 우상화 하다 싫다|\n| py_phbQxy5Y|  5|2022-04-29 18:44:10|              행복시티|         캔디 얌마 법 통과 시키다|\n| py_phbQxy5Y|  6|2022-04-29 18:44:11|                  왕19|  백 밥그릇 아니다 만주 당 편 ...|\n| py_phbQxy5Y|  7|2022-04-29 18:44:17|              행복시티|               캔디 유능하다 자샤|\n| py_phbQxy5Y|  8|2022-04-29 18:44:20|                고광재|     시티 야 그냥 미국 가다 아 ㅋ|\n| py_phbQxy5Y|  9|2022-04-29 18:44:22|                  캔디|     시티 아저씨 야 너 네 당 이여|\n| py_phbQxy5Y| 10|2022-04-29 18:44:31|                se nal| 국제 정세 한반도 위기 상황 보...|\n| py_phbQxy5Y| 11|2022-04-29 18:44:41|              행복시티| 캔디 합의 정신 발휘 하다 보다...|\n| py_phbQxy5Y| 12|2022-04-29 18:44:45|                  캔디|      시티 아저씨 야 그거 병 이야|\n| py_phbQxy5Y| 13|2022-04-29 18:44:47|              행복시티|                캔디 젠 밀다 붙다|\n| py_phbQxy5Y| 14|2022-04-29 18:44:48|                  왕19|역사상 이렇다 정당 거의 이기붕...|\n| py_phbQxy5Y| 15|2022-04-29 18:44:53|              이슬처럼|  적패 세력 척결 개인 영 달 부...|\n| py_phbQxy5Y| 16|2022-04-29 18:44:57|             hasinmajo| 윤가눔 임명 강행 면 되다 총리...|\n| py_phbQxy5Y| 17|2022-04-29 18:44:57|              행복시티|   캔디 니 180 석 고 뭐 하다 자샤|\n| py_phbQxy5Y| 18|2022-04-29 18:44:59|마늘냄새 나는 한국인들|                            애 아|\n| py_phbQxy5Y| 19|2022-04-29 18:45:00|                  캔디|                        병원 가다|\n+------------+---+-------------------+----------------------+---------------------------------+\nonly showing top 20 rows\n\n\r                                                                                \r"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1651281824401_1515849340",
      "id": "paragraph_1651281824401_1515849340",
      "dateCreated": "2022-04-30 01:23:44.401",
      "dateStarted": "2022-04-30 02:46:08.428",
      "dateFinished": "2022-04-30 02:46:23.255",
      "status": "FINISHED"
    },
    {
      "text": "%python\n\nimport pyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import StringType\n\n\nKAFKA_TOPIC \u003d \"crawling1\"\nKAFKA_SERVER \u003d \"localhost:9092\"\n\nspark_session \u003d SparkSession \\\n    .builder \\\n    .appName(\"kafka-to-spark\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n\ndf \u003d spark_session \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"subscribe\", \"input\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .load()\n\ndf1 \u003d df.selectExpr(\"CAST(value AS STRING)\")\n\nschema \u003d StructType([ \\\nStructField(\"video_unique\",StringType()), \\\nStructField(\"num\",IntegerType()), \\\nStructField(\"chat_time\",TimestampType()), \\\nStructField(\"chat_id\",StringType()), \\\nStructField(\"chat_message\",StringType()), \\\n])\n\ndf2 \u003d df1.select(from_json(\"value\",schema).alias(\"data\")).select(\"data.*\")\n\ndf2 \\\n.selectExpr(\"CAST(\u0027chat_id\u0027 AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n.writeStream \\\n.format(\u0027kafka\u0027) \\\n.outputMode(\"append\") \\\n.option(\u0027kafka.bootstrap.servers\u0027, \u0027kafka:9092\u0027) \\\n.option(\u0027topic\u0027, \u0027output\u0027) \\\n.option(\"truncate\", False).option(\"checkpointLocation\", \"/tmp/dtn/checkpoint\") \\\n.start().awaitTermination()\n",
      "user": "anonymous",
      "dateUpdated": "2022-04-30 01:05:39.622",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {
          "0": {
            "graph": {
              "mode": "table",
              "height": 646.0,
              "optionOpen": false
            }
          }
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "22/04/30 01:05:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark\u0027s default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/04/30 01:05:05 WARN Utils: Service \u0027SparkUI\u0027 could not bind on port 4040. Attempting port 4041.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\nInput \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m\u003ccell line: 41\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m schema \u001b[38;5;241m\u003d\u001b[39m StructType([ \\\n\u001b[1;32m     33\u001b[0m StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_unique\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType()), \\\n\u001b[1;32m     34\u001b[0m StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m,IntegerType()), \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_message\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType()), \\\n\u001b[1;32m     38\u001b[0m ])\n\u001b[1;32m     40\u001b[0m df2 \u001b[38;5;241m\u003d\u001b[39m df1\u001b[38;5;241m.\u001b[39mselect(from_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m,schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---\u003e 41\u001b[0m \u001b[43mdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\u0027\u001b[39m\u001b[38;5;124m/mycsv.csv\u001b[39m\u001b[38;5;124m\u0027\u001b[39m)\n\nFile \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:246\u001b[0m, in \u001b[0;36mDataFrame.write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    Interface for saving the content of the non-streaming :class:`DataFrame` out into external\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    storage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    :class:`DataFrameWriter`\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--\u003e 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py:543\u001b[0m, in \u001b[0;36mDataFrameWriter.__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m\u003d\u001b[39m df\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m\u003d\u001b[39m df\u001b[38;5;241m.\u001b[39msql_ctx\n\u001b[0;32m--\u003e 543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite \u001b[38;5;241m\u003d\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\nFile \u001b[0;32m/tmp/zeppelin_jupyter_kernel_python5558044557132512735/py4j-src-0.10.7.zip/py4j/java_gateway.py:1256\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1250\u001b[0m command \u001b[38;5;241m\u003d\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1252\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1253\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1255\u001b[0m answer \u001b[38;5;241m\u003d\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-\u003e 1256\u001b[0m return_value \u001b[38;5;241m\u003d\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1260\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n\nFile \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.\u003clocals\u003e.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m\u003d\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--\u003e 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\n\u001b[0;31mAnalysisException\u001b[0m: \u0027write\u0027 can not be called on streaming Dataset/DataFrame"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1651231165205_741230883",
      "id": "paragraph_1651231165205_741230883",
      "dateCreated": "2022-04-29 11:19:25.205",
      "dateStarted": "2022-04-30 01:04:57.138",
      "dateFinished": "2022-04-30 01:05:12.137",
      "status": "ERROR"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2022-04-29 11:20:03.860",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1651231203860_1843701454",
      "id": "paragraph_1651231203860_1843701454",
      "dateCreated": "2022-04-29 11:20:03.860",
      "status": "READY"
    }
  ],
  "name": "streamingspark",
  "id": "2H2U8M98Q",
  "defaultInterpreterGroup": "spark-submit",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}