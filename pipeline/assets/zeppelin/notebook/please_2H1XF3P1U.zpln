{
  "paragraphs": [
    {
      "text": "%pyspark\r\n\r\nfrom pyspark.sql.functions import *\r\nfrom pyspark.sql.types import *\r\nfrom pyspark import SparkConf\r\nfrom pyspark import SparkContext\r\nfrom pyspark.streaming import StreamingContext\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql import SQLContext\r\nfrom pyspark.sql.functions import explode\r\nfrom pyspark.sql.functions import desc\r\nfrom pyspark.sql.functions import split\r\nfrom pyspark.sql.types import StructType\r\nfrom pyspark.sql.types import StructField\r\nfrom pyspark.sql.types import IntegerType\r\nfrom pyspark.sql.types import StringType\r\nfrom pyspark.sql import SparkSession\r\n\r\nsparkConf \u003d SparkConf()\r\nconf.setMaster(\u0027spark://spark-master:7077\u0027)\r\nconf.setAppName(\u0027spark-streaming\u0027)\r\n\r\nspark_session \u003d SparkSession \\\r\n.builder \\\r\n.config(\"SparkConf\") \\\r\n.getOrCreate()\r\n\r\nsparkconf \u003d SparkConf() \\\r\n    .setMaster(\"spark://spark-master:7077\") \\\r\n    .setAppName(\u0027spark-basic\u0027) \\\r\n\r\ndf \u003d spark_session \\\r\n    .readStream \\\r\n    .format(\"kafka\") \\\r\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\r\n    .option(\"subscribe\", \"input\") \\\r\n    .option(\"startingOffsets\", \"earliest\") \\\r\n    .load()\r\n\r\ndf1 \u003d df.selectExpr(\"CAST(value AS STRING)\")\r\n\r\nschema \u003d StructType([ \\\r\nStructField(\"video_unique\",StringType(),True), \\\r\nStructField(\"num\",StringType(),True), \\\r\nStructField(\"chat_time\",TimestampType(),True), \\\r\nStructField(\"chat_id\",StringType(),True), \\\r\nStructField(\"chat_message\",StringType(),True), \r\n])\r\n\r\ndf2 \u003d df1.select(from_json(\"value\",schema).alias(\"data\")).select(\"data.*\")\r\n\r\ndf2.writeStream \\\r\n    .format(\"kafka\") \\\r\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\r\n    .option(\"topic\", \"output\") \\\r\n    .outputMode(\"complete\") \\\r\n    .option(\"checkpointLocation\", \"/streaming/checkpointLocation\") \\\r\n    .start()\r\n\r\ndf2.show()",
      "user": "anonymous",
      "dateUpdated": "2022-04-26 10:50:07.749",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n\u001b[0;32m/tmp/ipykernel_915/2959570652.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 55\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/streaming/checkpointLocation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value \u003d get_return_value(\n\u001b[0;32m-\u003e 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mAnalysisException\u001b[0m: Complete output mode not supported when there are no streaming aggregations on streaming DataFrames/Datasets;\nProject [data#338.video_unique AS video_unique#340, data#338.num AS num#341, data#338.chat_time AS chat_time#342, data#338.chat_id AS chat_id#343, data#338.chat_message AS chat_message#344]\n+- Project [from_json(StructField(video_unique,StringType,true), StructField(num,StringType,true), StructField(chat_time,TimestampType,true), StructField(chat_id,StringType,true), StructField(chat_message,StringType,true), value#336, Some(GMT)) AS data#338]\n   +- Project [cast(value#323 as string) AS value#336]\n      +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@b567823, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6988672e, [startingOffsets\u003dearliest, kafka.bootstrap.servers\u003dkafka:9092, subscribe\u003dinput], [key#322, value#323, topic#324, partition#325, offset#326L, timestamp#327, timestampType#328], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@7cdbd82d,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -\u003e kafka:9092, subscribe -\u003e input, startingOffsets -\u003e earliest),None), kafka, [key#315, value#316, topic#317, partition#318, offset#319L, timestamp#320, timestampType#321]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1650969068916_1274754830",
      "id": "paragraph_1650969068916_1274754830",
      "dateCreated": "2022-04-26 10:31:08.916",
      "dateStarted": "2022-04-26 10:50:07.769",
      "dateFinished": "2022-04-26 10:50:08.115",
      "status": "ERROR"
    },
    {
      "text": "%pyspark\n",
      "user": "anonymous",
      "dateUpdated": "2022-04-26 10:31:49.176",
      "progress": 0,
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1650969109175_352473865",
      "id": "paragraph_1650969109175_352473865",
      "dateCreated": "2022-04-26 10:31:49.176",
      "status": "READY"
    }
  ],
  "name": "please",
  "id": "2H1XF3P1U",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}