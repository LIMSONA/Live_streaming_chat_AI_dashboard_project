{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff032c8",
   "metadata": {},
   "source": [
    "# 0. 들어가기 앞서\n",
    "\n",
    "* 고객 질문: 1, 상담원 질문: 2, 고객 및 상담원 대답: 0\n",
    "* 참고: https://velog.io/@seolini43/KOBERT%EB%A1%9C-%EB%8B%A4%EC%A4%91-%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0-%ED%8C%8C%EC%9D%B4%EC%8D%ACColab\n",
    "\n",
    "* 한국어언어모델 다양하게 사용해보기 : https://littlefoxdiary.tistory.com/81"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6590be",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c03f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "# from tqdm.notebook import tqdm\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57393f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /anaconda/envs/version_test_azureml_py38/lib/python3.8/site-packages (1.22.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f00e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GPU 사용 시에\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c8ef6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27195632",
   "metadata": {},
   "source": [
    "# 2. 모델, 사전, 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d08d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/adminuser/notebooks/modeling/question/[1차] KoBERT_QA_자모음전처리/.cache/kobert_v1.zip\n",
      "using cached model. /home/adminuser/notebooks/modeling/question/[1차] KoBERT_QA_자모음전처리/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4d31c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '질의응답_K쇼핑_질문유형분류_자모음전처리.csv',\n",
       " '질의응답_K쇼핑_질문분류_자모음전처리.csv',\n",
       " '1_data_processing.ipynb']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../[1차] 자음모음_preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0662872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../[1차] 자음모음_preprocessing/질의응답_K쇼핑_질문분류_자모음전처리.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68210ad6",
   "metadata": {},
   "source": [
    "# 3. 질문분류시작\n",
    "\n",
    "* 고객 질문: 1, 상담원 질문: 2, 고객 및 상담원 대답: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c6fac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg</th>\n",
       "      <th>QA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>저는 입니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>네. 아쿠아 청소기를 샀었는데요.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  msg  QA\n",
       "0             저는 입니다.   0\n",
       "1  네. 아쿠아 청소기를 샀었는데요.   1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7182b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2중 리스트로 변환됨\n",
    "\n",
    "data_list = []\n",
    "for q, label in zip(df[\"msg\"],df[\"QA\"])  :\n",
    "    data = []\n",
    "    data.append(q)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ae42c",
   "metadata": {},
   "source": [
    "## 3-1. Train / Test set 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3766de",
   "metadata": {},
   "source": [
    "* 라벨링은 이미 진행했으므로, 바로 train/ test 분리 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189c8181",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c20346a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751035\n",
      "250345\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba4b04",
   "metadata": {},
   "source": [
    "## 3-2. KoBERT 입력 데이터로 만들기\n",
    "\n",
    "* 데이터를 train data와 test data로 나누었다면 각 데이터가 KoBERT 모델의 입력으로 들어갈 수 있는 형태가 되도록 토큰화, 정수 인코딩, 패딩 등을 해주어야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "325540ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f60101ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "\n",
    "max_len = 64 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d51a085",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/adminuser/notebooks/modeling/question/[1차] KoBERT_QA_자모음전처리/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "tokenizer= get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a38a513a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 1469,  517,   54,  994, 5778, 3990, 7993, 4398, 6903, 4680,\n",
       "        7736, 4004, 4981, 5330, 5330, 2391, 3155, 6553, 2443, 6116, 4756,\n",
       "        6553, 1239, 7088, 5130, 7088, 4926, 3136, 6116, 1788, 6060, 6844,\n",
       "        4931, 1434, 7132, 4223, 7111, 5130, 7794, 1706, 2964,  889, 6135,\n",
       "         905,  830, 6701,  517,   54,    3,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1], dtype=int32),\n",
       " array(50, dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       dtype=int32),\n",
       " 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째는 패딩된 시퀀스\n",
    "# 두 번째는 길이와 타입에 대한 내용\n",
    "# 세 번재는 어텐션 마스크 시퀀스\n",
    "\n",
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd75b304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a46aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch용 DataLoader 사용(torch 형식의 dataset을 만들어주기)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1b0fb",
   "metadata": {},
   "source": [
    "## 3-3. KoBERT 학습모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6cc27",
   "metadata": {},
   "source": [
    "* 고객 질문: 1, 상담원 질문: 2, 고객 및 상담원 대답: 0 \n",
    "* 3가지의 class를 분류하기 때문에 num_classes는 3으로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ba1174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=3,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6b75d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT 모델 불러오기\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee52fb4",
   "metadata": {},
   "source": [
    "## 3-4. KoBERT 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1f3486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28535/2673822008.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7442a3a228514a75ba7c2db4d02eab80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.1500736474990845 train acc 0.375\n",
      "epoch 1 batch id 201 loss 0.7987498641014099 train acc 0.49852300995024873\n",
      "epoch 1 batch id 401 loss 0.40835580229759216 train acc 0.671290523690773\n",
      "epoch 1 batch id 601 loss 0.352670818567276 train acc 0.753561772046589\n",
      "epoch 1 batch id 801 loss 0.2385759949684143 train acc 0.7960362047440699\n",
      "epoch 1 batch id 1001 loss 0.21680225431919098 train acc 0.8220529470529471\n",
      "epoch 1 batch id 1201 loss 0.22247156500816345 train acc 0.8403934221482098\n",
      "epoch 1 batch id 1401 loss 0.2968156039714813 train acc 0.8540551391862955\n",
      "epoch 1 batch id 1601 loss 0.19841614365577698 train acc 0.8644889912554653\n",
      "epoch 1 batch id 1801 loss 0.0808035358786583 train acc 0.8729698778456413\n",
      "epoch 1 batch id 2001 loss 0.12221647053956985 train acc 0.8794743253373314\n",
      "epoch 1 batch id 2201 loss 0.20200812816619873 train acc 0.8847185938209905\n",
      "epoch 1 batch id 2401 loss 0.19360895454883575 train acc 0.8891672740524781\n",
      "epoch 1 batch id 2601 loss 0.1879832148551941 train acc 0.8929017685505575\n",
      "epoch 1 batch id 2801 loss 0.1391933411359787 train acc 0.8963986076401286\n",
      "epoch 1 batch id 3001 loss 0.30074670910835266 train acc 0.8994137370876375\n",
      "epoch 1 batch id 3201 loss 0.10587508231401443 train acc 0.9020667369572009\n",
      "epoch 1 batch id 3401 loss 0.09958434104919434 train acc 0.9043066377536019\n",
      "epoch 1 batch id 3601 loss 0.28985536098480225 train acc 0.9062239655651207\n",
      "epoch 1 batch id 3801 loss 0.18548227846622467 train acc 0.9080669560641936\n",
      "epoch 1 batch id 4001 loss 0.2825677692890167 train acc 0.9095148087978006\n",
      "epoch 1 batch id 4201 loss 0.13790388405323029 train acc 0.9110777195905737\n",
      "epoch 1 batch id 4401 loss 0.14062495529651642 train acc 0.9124169222903885\n",
      "epoch 1 batch id 4601 loss 0.14593550562858582 train acc 0.9134325418387307\n",
      "epoch 1 batch id 4801 loss 0.14971204102039337 train acc 0.9144318891897522\n",
      "epoch 1 batch id 5001 loss 0.05586876720190048 train acc 0.9153044391121775\n",
      "epoch 1 batch id 5201 loss 0.17618870735168457 train acc 0.916130912324553\n",
      "epoch 1 batch id 5401 loss 0.08215036243200302 train acc 0.9170755415663766\n",
      "epoch 1 batch id 5601 loss 0.15161356329917908 train acc 0.9180698759150152\n",
      "epoch 1 batch id 5801 loss 0.053160659968853 train acc 0.9188178762282365\n",
      "epoch 1 batch id 6001 loss 0.06342902034521103 train acc 0.9195654890851525\n",
      "epoch 1 batch id 6201 loss 0.20742543041706085 train acc 0.9202825149169489\n",
      "epoch 1 batch id 6401 loss 0.24168764054775238 train acc 0.9210792259022028\n",
      "epoch 1 batch id 6601 loss 0.11842552572488785 train acc 0.9217448113922133\n",
      "epoch 1 batch id 6801 loss 0.08754096925258636 train acc 0.922352870901338\n",
      "epoch 1 batch id 7001 loss 0.14662329852581024 train acc 0.9229083345236395\n",
      "epoch 1 batch id 7201 loss 0.17624624073505402 train acc 0.9234307735036801\n",
      "epoch 1 batch id 7401 loss 0.1528313010931015 train acc 0.9239545331711931\n",
      "epoch 1 batch id 7601 loss 0.15199635922908783 train acc 0.9244671753716617\n",
      "epoch 1 batch id 7801 loss 0.16046036779880524 train acc 0.9249174785283938\n",
      "epoch 1 batch id 8001 loss 0.20949870347976685 train acc 0.9253491751031121\n",
      "epoch 1 batch id 8201 loss 0.1615864485502243 train acc 0.9257788684306791\n",
      "epoch 1 batch id 8401 loss 0.16997255384922028 train acc 0.9260969676229021\n",
      "epoch 1 batch id 8601 loss 0.09301306307315826 train acc 0.9265274386699222\n",
      "epoch 1 batch id 8801 loss 0.16323037445545197 train acc 0.9269347943415521\n",
      "epoch 1 batch id 9001 loss 0.16464702785015106 train acc 0.9273414065103878\n",
      "epoch 1 batch id 9201 loss 0.15889377892017365 train acc 0.9276827926312358\n",
      "epoch 1 batch id 9401 loss 0.24595846235752106 train acc 0.9281027284331455\n",
      "epoch 1 batch id 9601 loss 0.1687304973602295 train acc 0.9283684642224769\n",
      "epoch 1 batch id 9801 loss 0.1343902349472046 train acc 0.9286711815120906\n",
      "epoch 1 batch id 10001 loss 0.2786920368671417 train acc 0.9290258474152585\n",
      "epoch 1 batch id 10201 loss 0.2615506947040558 train acc 0.9293145279874522\n",
      "epoch 1 batch id 10401 loss 0.15514306724071503 train acc 0.9296281607537736\n",
      "epoch 1 batch id 10601 loss 0.1849769949913025 train acc 0.9298076242807283\n",
      "epoch 1 batch id 10801 loss 0.15227840840816498 train acc 0.9301062980279604\n",
      "epoch 1 batch id 11001 loss 0.1994781494140625 train acc 0.930327356603945\n",
      "epoch 1 batch id 11201 loss 0.11241642385721207 train acc 0.9305335461119543\n",
      "epoch 1 batch id 11401 loss 0.18200021982192993 train acc 0.9307119441277081\n",
      "epoch 1 batch id 11601 loss 0.09684425592422485 train acc 0.9309865528833722\n",
      "epoch 1 train acc 0.9311549002332585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28535/2673822008.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546e3f4a694a43e0bdfc1312d719d2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.9468634852424062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a83ff25ef542f793b98d5ade443408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.07345065474510193 train acc 0.984375\n",
      "epoch 2 batch id 201 loss 0.1601363867521286 train acc 0.9438743781094527\n",
      "epoch 2 batch id 401 loss 0.2376118302345276 train acc 0.9446695760598504\n",
      "epoch 2 batch id 601 loss 0.284757524728775 train acc 0.9460014559068219\n",
      "epoch 2 batch id 801 loss 0.10044948756694794 train acc 0.9454783083645443\n",
      "epoch 2 batch id 1001 loss 0.1155799999833107 train acc 0.9446491008991009\n",
      "epoch 2 batch id 1201 loss 0.16350264847278595 train acc 0.9442651956702748\n",
      "epoch 2 batch id 1401 loss 0.14466454088687897 train acc 0.9446266059957173\n",
      "epoch 2 batch id 1601 loss 0.16562150418758392 train acc 0.9448489225484072\n",
      "epoch 2 batch id 1801 loss 0.05261862277984619 train acc 0.9451172959466962\n",
      "epoch 2 batch id 2001 loss 0.14288562536239624 train acc 0.9452695527236382\n",
      "epoch 2 batch id 2201 loss 0.1508970558643341 train acc 0.94525215810995\n",
      "epoch 2 batch id 2401 loss 0.21057377755641937 train acc 0.9451270304039984\n",
      "epoch 2 batch id 2601 loss 0.20999878644943237 train acc 0.9451953575547867\n",
      "epoch 2 batch id 2801 loss 0.112563356757164 train acc 0.9453989646554802\n",
      "epoch 2 batch id 3001 loss 0.2847729027271271 train acc 0.9454296484505165\n",
      "epoch 2 batch id 3201 loss 0.0918087437748909 train acc 0.9455785301468291\n",
      "epoch 2 batch id 3401 loss 0.10930508375167847 train acc 0.9456961187885916\n",
      "epoch 2 batch id 3601 loss 0.23174622654914856 train acc 0.9456010483199111\n",
      "epoch 2 batch id 3801 loss 0.1533583253622055 train acc 0.945811957379637\n",
      "epoch 2 batch id 4001 loss 0.21447566151618958 train acc 0.9457596538365408\n",
      "epoch 2 batch id 4201 loss 0.13184843957424164 train acc 0.9458945786717449\n",
      "epoch 2 batch id 4401 loss 0.14111961424350739 train acc 0.9459781867757328\n",
      "epoch 2 batch id 4601 loss 0.13710926473140717 train acc 0.945894914149098\n",
      "epoch 2 batch id 4801 loss 0.09853816032409668 train acc 0.945906451780879\n",
      "epoch 2 batch id 5001 loss 0.047603484243154526 train acc 0.9457639722055589\n",
      "epoch 2 batch id 5201 loss 0.1223929300904274 train acc 0.9457916746779466\n",
      "epoch 2 batch id 5401 loss 0.037089478224515915 train acc 0.945967760599889\n",
      "epoch 2 batch id 5601 loss 0.1445932537317276 train acc 0.9461619576861274\n",
      "epoch 2 batch id 5801 loss 0.05212680250406265 train acc 0.9462835071539389\n",
      "epoch 2 batch id 6001 loss 0.04195799306035042 train acc 0.9463526912181303\n",
      "epoch 2 batch id 6201 loss 0.14167995750904083 train acc 0.9464476495726496\n",
      "epoch 2 batch id 6401 loss 0.3011097013950348 train acc 0.9466001406030308\n",
      "epoch 2 batch id 6601 loss 0.15078021585941315 train acc 0.9466487085290107\n",
      "epoch 2 batch id 6801 loss 0.04991715028882027 train acc 0.946676040288193\n",
      "epoch 2 batch id 7001 loss 0.14880065619945526 train acc 0.9466549421511212\n",
      "epoch 2 batch id 7201 loss 0.2100810408592224 train acc 0.9466870920705458\n",
      "epoch 2 batch id 7401 loss 0.12969525158405304 train acc 0.9467723956222133\n",
      "epoch 2 batch id 7601 loss 0.10955073684453964 train acc 0.9467894849361926\n",
      "epoch 2 batch id 7801 loss 0.15476754307746887 train acc 0.9468077009357775\n",
      "epoch 2 batch id 8001 loss 0.21291323006153107 train acc 0.9468679696287964\n",
      "epoch 2 batch id 8201 loss 0.12670767307281494 train acc 0.9469538775759053\n",
      "epoch 2 batch id 8401 loss 0.18251891434192657 train acc 0.9469668789429829\n",
      "epoch 2 batch id 8601 loss 0.11003661155700684 train acc 0.9470828246715498\n",
      "epoch 2 batch id 8801 loss 0.1031009778380394 train acc 0.9471633195091467\n",
      "epoch 2 batch id 9001 loss 0.14010301232337952 train acc 0.9472523886234863\n",
      "epoch 2 batch id 9201 loss 0.0992061197757721 train acc 0.9472577708944679\n",
      "epoch 2 batch id 9401 loss 0.2424093335866928 train acc 0.9473443649611744\n",
      "epoch 2 batch id 9601 loss 0.17986300587654114 train acc 0.9473345875429643\n",
      "epoch 2 batch id 9801 loss 0.1077100858092308 train acc 0.947317238036935\n",
      "epoch 2 batch id 10001 loss 0.1691943109035492 train acc 0.9473771372862714\n",
      "epoch 2 batch id 10201 loss 0.2575947344303131 train acc 0.9474270292128223\n",
      "epoch 2 batch id 10401 loss 0.18863654136657715 train acc 0.9475065498509758\n",
      "epoch 2 batch id 10601 loss 0.1708831787109375 train acc 0.9474857914347703\n",
      "epoch 2 batch id 10801 loss 0.1383730173110962 train acc 0.9475410262938617\n",
      "epoch 2 batch id 11001 loss 0.11281176656484604 train acc 0.9475246568493774\n",
      "epoch 2 batch id 11201 loss 0.09662564098834991 train acc 0.9475576957414517\n",
      "epoch 2 batch id 11401 loss 0.16759663820266724 train acc 0.9475498311551618\n",
      "epoch 2 batch id 11601 loss 0.07071636617183685 train acc 0.9476311309369881\n",
      "epoch 2 train acc 0.9476719965263987\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf79ea9917d4b5ea883d91d5f92b2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.9487806631565165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7b02401c3648b991f4676c0b66a248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.08281860500574112 train acc 0.984375\n",
      "epoch 3 batch id 201 loss 0.12958069145679474 train acc 0.9509483830845771\n",
      "epoch 3 batch id 401 loss 0.22628234326839447 train acc 0.9505922693266833\n",
      "epoch 3 batch id 601 loss 0.20733550190925598 train acc 0.9510191347753744\n",
      "epoch 3 batch id 801 loss 0.13298141956329346 train acc 0.9509597378277154\n",
      "epoch 3 batch id 1001 loss 0.08878012001514435 train acc 0.9500655594405595\n",
      "epoch 3 batch id 1201 loss 0.15067940950393677 train acc 0.9494822023313905\n",
      "epoch 3 batch id 1401 loss 0.12806043028831482 train acc 0.9502587437544611\n",
      "epoch 3 batch id 1601 loss 0.13606153428554535 train acc 0.950577763897564\n",
      "epoch 3 batch id 1801 loss 0.07490627467632294 train acc 0.9506610910605219\n",
      "epoch 3 batch id 2001 loss 0.1054530143737793 train acc 0.9507824212893553\n",
      "epoch 3 batch id 2201 loss 0.1346980482339859 train acc 0.9507823148568832\n",
      "epoch 3 batch id 2401 loss 0.14037488400936127 train acc 0.9506976259891712\n",
      "epoch 3 batch id 2601 loss 0.21114148199558258 train acc 0.9508782679738562\n",
      "epoch 3 batch id 2801 loss 0.11980250477790833 train acc 0.9510163780792574\n",
      "epoch 3 batch id 3001 loss 0.2656105160713196 train acc 0.9510111212929023\n",
      "epoch 3 batch id 3201 loss 0.0719323456287384 train acc 0.951021165260856\n",
      "epoch 3 batch id 3401 loss 0.09801581501960754 train acc 0.9510805645398412\n",
      "epoch 3 batch id 3601 loss 0.20101742446422577 train acc 0.9511116703693419\n",
      "epoch 3 batch id 3801 loss 0.1006552129983902 train acc 0.9512340502499342\n",
      "epoch 3 batch id 4001 loss 0.22705328464508057 train acc 0.9512621844538866\n",
      "epoch 3 batch id 4201 loss 0.11888062953948975 train acc 0.9514029397762438\n",
      "epoch 3 batch id 4401 loss 0.12276218086481094 train acc 0.9514243921835946\n",
      "epoch 3 batch id 4601 loss 0.13235865533351898 train acc 0.95138285155401\n",
      "epoch 3 batch id 4801 loss 0.07288873195648193 train acc 0.9513805717558842\n",
      "epoch 3 batch id 5001 loss 0.079310841858387 train acc 0.9512691211757649\n",
      "epoch 3 batch id 5201 loss 0.13662008941173553 train acc 0.9512653816573736\n",
      "epoch 3 batch id 5401 loss 0.02972119301557541 train acc 0.9515251805221255\n",
      "epoch 3 batch id 5601 loss 0.1436411291360855 train acc 0.9517190010712373\n",
      "epoch 3 batch id 5801 loss 0.0640309527516365 train acc 0.9517432339251853\n",
      "epoch 3 batch id 6001 loss 0.04694656655192375 train acc 0.9518309448425263\n",
      "epoch 3 batch id 6201 loss 0.1396085023880005 train acc 0.9519255966779552\n",
      "epoch 3 batch id 6401 loss 0.1948539763689041 train acc 0.9521144157162943\n",
      "epoch 3 batch id 6601 loss 0.09884024411439896 train acc 0.9522160468110892\n",
      "epoch 3 batch id 6801 loss 0.03056357614696026 train acc 0.9522266946037348\n",
      "epoch 3 batch id 7001 loss 0.1094934418797493 train acc 0.9521965612055421\n",
      "epoch 3 batch id 7201 loss 0.15292520821094513 train acc 0.9522136682405221\n",
      "epoch 3 batch id 7401 loss 0.11137457937002182 train acc 0.952305853938657\n",
      "epoch 3 batch id 7601 loss 0.07731487601995468 train acc 0.9523829101434022\n",
      "epoch 3 batch id 7801 loss 0.11991982161998749 train acc 0.9524159562876554\n",
      "epoch 3 batch id 8001 loss 0.19346770644187927 train acc 0.9525098425196851\n",
      "epoch 3 batch id 8201 loss 0.0872175395488739 train acc 0.9525705706621144\n",
      "epoch 3 batch id 8401 loss 0.17033277451992035 train acc 0.952529832758005\n",
      "epoch 3 batch id 8601 loss 0.06876315921545029 train acc 0.9526490379025695\n",
      "epoch 3 batch id 8801 loss 0.08871977776288986 train acc 0.9527361947505966\n",
      "epoch 3 batch id 9001 loss 0.10813829302787781 train acc 0.9528854432840795\n",
      "epoch 3 batch id 9201 loss 0.07622269541025162 train acc 0.9529365014672319\n",
      "epoch 3 batch id 9401 loss 0.2416413426399231 train acc 0.9530618418253377\n",
      "epoch 3 batch id 9601 loss 0.1738225370645523 train acc 0.9530908238725133\n",
      "epoch 3 batch id 9801 loss 0.08379858732223511 train acc 0.9530899270482603\n",
      "epoch 3 batch id 10001 loss 0.0950738713145256 train acc 0.9532171782821718\n",
      "epoch 3 batch id 10201 loss 0.25403642654418945 train acc 0.953256727281639\n",
      "epoch 3 batch id 10401 loss 0.17629726231098175 train acc 0.9533518411691183\n",
      "epoch 3 batch id 10601 loss 0.1746309995651245 train acc 0.9533048179417036\n",
      "epoch 3 batch id 10801 loss 0.11505776643753052 train acc 0.9533680330524952\n",
      "epoch 3 batch id 11001 loss 0.08715232461690903 train acc 0.9533337878374694\n",
      "epoch 3 batch id 11201 loss 0.09536480158567429 train acc 0.9533579591107937\n",
      "epoch 3 batch id 11401 loss 0.13749758899211884 train acc 0.9533497609858784\n",
      "epoch 3 batch id 11601 loss 0.06844992935657501 train acc 0.9534522885958107\n",
      "epoch 3 train acc 0.9534904818989983\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e68cf55f9c48cfa50d75e886d1750b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.9483133510399522\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dc2b877d004561a06ff264579e1d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.0873342901468277 train acc 0.96875\n",
      "epoch 4 batch id 201 loss 0.13216979801654816 train acc 0.959032960199005\n",
      "epoch 4 batch id 401 loss 0.1664436161518097 train acc 0.9574501246882793\n",
      "epoch 4 batch id 601 loss 0.27084821462631226 train acc 0.9576487104825291\n",
      "epoch 4 batch id 801 loss 0.09592141211032867 train acc 0.9578846754057428\n",
      "epoch 4 batch id 1001 loss 0.0699266716837883 train acc 0.9573083166833167\n",
      "epoch 4 batch id 1201 loss 0.14445099234580994 train acc 0.9567808076602831\n",
      "epoch 4 batch id 1401 loss 0.12305692583322525 train acc 0.9573741970021413\n",
      "epoch 4 batch id 1601 loss 0.06420314311981201 train acc 0.9574387101811368\n",
      "epoch 4 batch id 1801 loss 0.09686057269573212 train acc 0.9574715435868961\n",
      "epoch 4 batch id 2001 loss 0.10195808112621307 train acc 0.9574665792103948\n",
      "epoch 4 batch id 2201 loss 0.09202592819929123 train acc 0.9574057246706043\n",
      "epoch 4 batch id 2401 loss 0.12231045961380005 train acc 0.9573745314452311\n",
      "epoch 4 batch id 2601 loss 0.1396239697933197 train acc 0.9575343617839293\n",
      "epoch 4 batch id 2801 loss 0.11498234421014786 train acc 0.9575988486254909\n",
      "epoch 4 batch id 3001 loss 0.2649713456630707 train acc 0.9576026741086304\n",
      "epoch 4 batch id 3201 loss 0.10084130614995956 train acc 0.9576450718525461\n",
      "epoch 4 batch id 3401 loss 0.08269019424915314 train acc 0.9577513966480447\n",
      "epoch 4 batch id 3601 loss 0.1726740300655365 train acc 0.9577721466259372\n",
      "epoch 4 batch id 3801 loss 0.0701814517378807 train acc 0.9578811496974481\n",
      "epoch 4 batch id 4001 loss 0.25728315114974976 train acc 0.9578425706073481\n",
      "epoch 4 batch id 4201 loss 0.15156681835651398 train acc 0.9580085098786003\n",
      "epoch 4 batch id 4401 loss 0.16098755598068237 train acc 0.9579818507157464\n",
      "epoch 4 batch id 4601 loss 0.11031351238489151 train acc 0.9580220332536405\n",
      "epoch 4 batch id 4801 loss 0.08434472978115082 train acc 0.9581499947927515\n",
      "epoch 4 batch id 5001 loss 0.029239466413855553 train acc 0.9580208958208358\n",
      "epoch 4 batch id 5201 loss 0.10834857076406479 train acc 0.9580309075177851\n",
      "epoch 4 batch id 5401 loss 0.02096865139901638 train acc 0.9582918672468062\n",
      "epoch 4 batch id 5601 loss 0.1202683299779892 train acc 0.9585090831994286\n",
      "epoch 4 batch id 5801 loss 0.040542811155319214 train acc 0.9585604852611619\n",
      "epoch 4 batch id 6001 loss 0.033948320895433426 train acc 0.9587438551908015\n",
      "epoch 4 batch id 6201 loss 0.13617222011089325 train acc 0.9588372843089824\n",
      "epoch 4 batch id 6401 loss 0.1360214203596115 train acc 0.9589907826902047\n",
      "epoch 4 batch id 6601 loss 0.0540749728679657 train acc 0.959168118466899\n",
      "epoch 4 batch id 6801 loss 0.03425343334674835 train acc 0.9591948794294957\n",
      "epoch 4 batch id 7001 loss 0.11386999487876892 train acc 0.9591710112841023\n",
      "epoch 4 batch id 7201 loss 0.16582538187503815 train acc 0.9592265831134564\n",
      "epoch 4 batch id 7401 loss 0.09433914721012115 train acc 0.9593255978921768\n",
      "epoch 4 batch id 7601 loss 0.03681615740060806 train acc 0.9593988455466386\n",
      "epoch 4 batch id 7801 loss 0.11624611914157867 train acc 0.9594863639277016\n",
      "epoch 4 batch id 8001 loss 0.1524960696697235 train acc 0.9595460723659542\n",
      "epoch 4 batch id 8201 loss 0.05198429524898529 train acc 0.9596600262163151\n",
      "epoch 4 batch id 8401 loss 0.16969238221645355 train acc 0.9596532406856326\n",
      "epoch 4 batch id 8601 loss 0.07943090796470642 train acc 0.9597466864318103\n",
      "epoch 4 batch id 8801 loss 0.06747729331254959 train acc 0.9598039285308487\n",
      "epoch 4 batch id 9001 loss 0.08317123353481293 train acc 0.9599679896678147\n",
      "epoch 4 batch id 9201 loss 0.04221729189157486 train acc 0.9600349146831866\n",
      "epoch 4 batch id 9401 loss 0.21381770074367523 train acc 0.9601521779597915\n",
      "epoch 4 batch id 9601 loss 0.1435524970293045 train acc 0.9601929486511822\n",
      "epoch 4 batch id 9801 loss 0.07803419977426529 train acc 0.9601890113253749\n",
      "epoch 4 batch id 10001 loss 0.09909521043300629 train acc 0.9602711603839617\n",
      "epoch 4 batch id 10201 loss 0.22759388387203217 train acc 0.9603255808254093\n",
      "epoch 4 batch id 10401 loss 0.16167694330215454 train acc 0.9604289851937313\n",
      "epoch 4 batch id 10601 loss 0.18098095059394836 train acc 0.9604120483916612\n",
      "epoch 4 batch id 10801 loss 0.11933481693267822 train acc 0.9604724099620405\n",
      "epoch 4 batch id 11001 loss 0.08405690640211105 train acc 0.9604240523588765\n",
      "epoch 4 batch id 11201 loss 0.07666144520044327 train acc 0.9604248504597803\n",
      "epoch 4 batch id 11401 loss 0.14434008300304413 train acc 0.9604475484606614\n",
      "epoch 4 batch id 11601 loss 0.03138993680477142 train acc 0.9605610507714852\n",
      "epoch 4 train acc 0.9606139373740729\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879fad82bcab48bdbb204f1661c33a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 test acc 0.9486528512955759\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a389497a2584b3d9d39d5dca879e21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.0710766613483429 train acc 0.96875\n",
      "epoch 5 batch id 201 loss 0.08026131987571716 train acc 0.9674284825870647\n",
      "epoch 5 batch id 401 loss 0.13890217244625092 train acc 0.9655548628428927\n",
      "epoch 5 batch id 601 loss 0.2610434591770172 train acc 0.9655522046589018\n",
      "epoch 5 batch id 801 loss 0.06589925289154053 train acc 0.9648876404494382\n",
      "epoch 5 batch id 1001 loss 0.0913609191775322 train acc 0.9646291208791209\n",
      "epoch 5 batch id 1201 loss 0.08098041266202927 train acc 0.9641965029142381\n",
      "epoch 5 batch id 1401 loss 0.11603882908821106 train acc 0.9648130799428979\n",
      "epoch 5 batch id 1601 loss 0.04840148985385895 train acc 0.964787632729544\n",
      "epoch 5 batch id 1801 loss 0.10716582089662552 train acc 0.9648198917268185\n",
      "epoch 5 batch id 2001 loss 0.06180320680141449 train acc 0.964923788105947\n",
      "epoch 5 batch id 2201 loss 0.07981313765048981 train acc 0.964781633348478\n",
      "epoch 5 batch id 2401 loss 0.10174062848091125 train acc 0.9646956997084548\n",
      "epoch 5 batch id 2601 loss 0.12959755957126617 train acc 0.9647551422529796\n",
      "epoch 5 batch id 2801 loss 0.12278559058904648 train acc 0.964817252766869\n",
      "epoch 5 batch id 3001 loss 0.2790212631225586 train acc 0.9647461262912362\n",
      "epoch 5 batch id 3201 loss 0.1161251962184906 train acc 0.9647766323024055\n",
      "epoch 5 batch id 3401 loss 0.08753160387277603 train acc 0.9648816524551602\n",
      "epoch 5 batch id 3601 loss 0.1812361478805542 train acc 0.964909920855318\n",
      "epoch 5 batch id 3801 loss 0.05459602549672127 train acc 0.9650420941857406\n",
      "epoch 5 batch id 4001 loss 0.22193217277526855 train acc 0.9651141902024494\n",
      "epoch 5 batch id 4201 loss 0.12914900481700897 train acc 0.9652649666746013\n",
      "epoch 5 batch id 4401 loss 0.11585164070129395 train acc 0.9652919790956601\n",
      "epoch 5 batch id 4601 loss 0.08430534601211548 train acc 0.9652385351010649\n",
      "epoch 5 batch id 4801 loss 0.03708914667367935 train acc 0.9653490158300354\n",
      "epoch 5 batch id 5001 loss 0.013415086083114147 train acc 0.965241326734653\n",
      "epoch 5 batch id 5201 loss 0.12658043205738068 train acc 0.9651569409728898\n",
      "epoch 5 batch id 5401 loss 0.017488984391093254 train acc 0.9653218154045548\n",
      "epoch 5 batch id 5601 loss 0.0978439673781395 train acc 0.9655307088019996\n",
      "epoch 5 batch id 5801 loss 0.033199213445186615 train acc 0.9655635881744526\n",
      "epoch 5 batch id 6001 loss 0.04135676100850105 train acc 0.9656619730044993\n",
      "epoch 5 batch id 6201 loss 0.12291619181632996 train acc 0.965721254636349\n",
      "epoch 5 batch id 6401 loss 0.1410413682460785 train acc 0.9658525035150758\n",
      "epoch 5 batch id 6601 loss 0.05159001797437668 train acc 0.9659544955309801\n",
      "epoch 5 batch id 6801 loss 0.019889365881681442 train acc 0.966011432142332\n",
      "epoch 5 batch id 7001 loss 0.09823665022850037 train acc 0.9659423653763748\n",
      "epoch 5 batch id 7201 loss 0.1385636329650879 train acc 0.966018174559089\n",
      "epoch 5 batch id 7401 loss 0.11411616951227188 train acc 0.9660941089042021\n",
      "epoch 5 batch id 7601 loss 0.04487742856144905 train acc 0.9661249342191817\n",
      "epoch 5 batch id 7801 loss 0.08527546375989914 train acc 0.966084075759518\n",
      "epoch 5 batch id 8001 loss 0.09282364696264267 train acc 0.9661038463942008\n",
      "epoch 5 batch id 8201 loss 0.04051855579018593 train acc 0.9661512315571272\n",
      "epoch 5 batch id 8401 loss 0.1105935350060463 train acc 0.9661591625996905\n",
      "epoch 5 batch id 8601 loss 0.09752610325813293 train acc 0.9662175909777933\n",
      "epoch 5 batch id 8801 loss 0.03138856589794159 train acc 0.9662325303942734\n",
      "epoch 5 batch id 9001 loss 0.06843410432338715 train acc 0.9663110348850128\n",
      "epoch 5 batch id 9201 loss 0.022690456360578537 train acc 0.966341973698511\n",
      "epoch 5 batch id 9401 loss 0.21746200323104858 train acc 0.9664048372513563\n",
      "epoch 5 batch id 9601 loss 0.12543973326683044 train acc 0.966396729507343\n",
      "epoch 5 batch id 9801 loss 0.046382274478673935 train acc 0.9664176487093153\n",
      "epoch 5 batch id 10001 loss 0.07408861815929413 train acc 0.9664658534146585\n",
      "epoch 5 batch id 10201 loss 0.20508897304534912 train acc 0.9664937873737869\n",
      "epoch 5 batch id 10401 loss 0.16678941249847412 train acc 0.9665431809441399\n",
      "epoch 5 batch id 10601 loss 0.1645933836698532 train acc 0.9665125931515894\n",
      "epoch 5 batch id 10801 loss 0.14617660641670227 train acc 0.9665627025275437\n",
      "epoch 5 batch id 11001 loss 0.06762341409921646 train acc 0.9664973638760113\n",
      "epoch 5 batch id 11201 loss 0.05436059087514877 train acc 0.9664315686099455\n",
      "epoch 5 batch id 11401 loss 0.1606348752975464 train acc 0.9664283834751337\n",
      "epoch 5 batch id 11601 loss 0.03345233574509621 train acc 0.9664845702956641\n",
      "epoch 5 train acc 0.9665017729087981\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f542f5dad9b148188074a26210da1114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 test acc 0.9485392624008678\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dee8b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "#모델의 형태를 포함하여 저장하기\n",
    "torch.save(model, 'KoBERT_QA_v.0.1.1_sona.pth')\n",
    "\n",
    "#불러오기\n",
    "# model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf52910",
   "metadata": {},
   "source": [
    "## 3-5.새로운 문장 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6470ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/adminuser/notebooks/modeling/question/[1차] KoBERT_QA_자모음전처리/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "\n",
    "        test_eval=[]\n",
    "        for i in out:\n",
    "            logits=i\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            # 고객 질문: 1, 상담원 질문: 2, 고객 및 상담원 대답: 0\n",
    "\n",
    "            if np.argmax(logits) == 0:\n",
    "                test_eval.append(\"대답\")\n",
    "            elif np.argmax(logits) == 1:\n",
    "                test_eval.append(\"고객 질문\")\n",
    "            elif np.argmax(logits) == 2:\n",
    "                test_eval.append(\"상담원 질문\")\n",
    "\n",
    "        print(\">> 입력하신 내용은 \" + test_eval[0] + \" 라고 판단됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "983be80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하고싶은 말을 입력해주세요 : 지금 몇시야??\n",
      ">> 입력하신 내용은 고객 질문 라고 판단됩니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 언제 방송 끝나냥??\n",
      ">> 입력하신 내용은 고객 질문 라고 판단됩니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 지금 분홍색 말씀하신 것 맞죠?\n",
      ">> 입력하신 내용은 상담원 질문 라고 판단됩니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 언제 가능한가요?\n",
      ">> 입력하신 내용은 고객 질문 라고 판단됩니다.\n",
      "\n",
      "\n",
      "하고싶은 말을 입력해주세요 : 헐렁한 것을 원하시나요?\n",
      ">> 입력하신 내용은 상담원 질문 라고 판단됩니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "end = 1\n",
    "while end == 1 :\n",
    "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
    "    if sentence == 0 :\n",
    "        break\n",
    "    predict(sentence)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89c08c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "version_test_azureml_py38",
   "language": "python",
   "name": "conda-env-version_test_azureml_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
