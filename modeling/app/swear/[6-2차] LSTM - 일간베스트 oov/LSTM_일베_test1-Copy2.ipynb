{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f702e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 05:55:01.121313: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-05-25 05:55:01.152240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0001:00:00.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-05-25 05:55:01.152412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-25 05:55:01.153797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-25 05:55:01.155246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-25 05:55:01.155481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-25 05:55:01.156937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-25 05:55:01.157747: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-25 05:55:01.157912: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2022-05-25 05:55:01.157924: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-25 05:55:01.158358: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-05-25 05:55:01.164263: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2445435000 Hz\n",
      "2022-05-25 05:55:01.164479: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f632c000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-05-25 05:55:01.164497: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-05-25 05:55:01.165715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-05-25 05:55:01.165727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "###### 모델 불러오기 ######\n",
    "\n",
    "loaded_model = load_model('./test-model1.h5')\n",
    "\n",
    "###########################\n",
    "\n",
    "okt = Okt()\n",
    "tokenizer = Tokenizer()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','을','으로','자','에','와','한','하다','부터']\n",
    "max_len = 30\n",
    "vocab_size = 10000\n",
    "# tokenizer = Tokenizer(vocab_size)\n",
    "\n",
    "\n",
    "#############################\n",
    "# tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(new_sentence)\n",
    "\n",
    "###### json 파일 불러오기 ######\n",
    "\n",
    "with open('../일베집합_test4.json', encoding='utf-8') as json_file:\n",
    "    vocab = json.load(json_file)\n",
    "    tokenizer.word_index = vocab\n",
    "    \n",
    "################################\n",
    "\n",
    "def slang_predict(new_sentence):\n",
    "#     new_sentence = []\n",
    "#     tokenizer = Tokenizer(vocab_size, oov_token='OOV') #OOV 토큰\n",
    "#     tokenizer.fit_on_texts(new_sentence)\n",
    "#     train_input = tokenizer.texts_to_sequences(new_sentence)\n",
    "#     word_index = tokenizer.word_index(new_sentence)\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', str(new_sentence))\n",
    "    if new_sentence == \"\" or new_sentence.isspace():\n",
    "        return '일반 댓글'\n",
    "    else : \n",
    "        new_sentence = okt.morphs(new_sentence, stem=True)\n",
    "        new_sentence = [word for word in new_sentence if not word in stopwords]\n",
    "        if not new_sentence:\n",
    "            return '일반 댓글'\n",
    "        else:\n",
    "            encoded = tokenizer.texts_to_sequences([new_sentence])\n",
    "            pad_new = pad_sequences(encoded, maxlen = max_len)\n",
    "            score = loaded_model.predict(pad_new)\n",
    "            if(score > 0.7): \n",
    "                return \"{0}, 비속어 포함\".format(round(float(score), 2))\n",
    "            else:\n",
    "                return \"{0}, 일반 댓글\".format(round(float(1 - score), 2))\n",
    "\n",
    "# word_to_index = {}\n",
    "# word_to_index['OOV'] = len(word_to_index) + 1\n",
    "# print(word_to_index)\n",
    "# encoded_sentences = []\n",
    "\n",
    "# for sentence in preprocessed_sentences:\n",
    "#     encoded_sentence = []\n",
    "#     for word in sentence:\n",
    "#         try:\n",
    "#             # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
    "#             encoded_sentence.append(word_to_index[word])\n",
    "#         except KeyError:\n",
    "#             # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
    "#             encoded_sentence.append(word_to_index['OOV'])\n",
    "#     encoded_sentences.append(encoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b59c795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[0,24] = 6814 is not in [0, 2908)\n\t [[node sequential_9/embedding_9/embedding_lookup (defined at tmp/ipykernel_31357/1176218959.py:58) ]] [Op:__inference_predict_function_1162]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_9/embedding_9/embedding_lookup:\n sequential_9/embedding_9/embedding_lookup/707 (defined at anaconda/envs/check_up_py38_PT_TF/lib/python3.8/contextlib.py:113)\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mslang_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m근데 저게 이뻐보이나? 저 가격이면 쿠팡에서 두개 사고도 남음 \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mslang_predict\u001b[0;34m(new_sentence)\u001b[0m\n\u001b[1;32m     56\u001b[0m encoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([new_sentence])\n\u001b[1;32m     57\u001b[0m pad_new \u001b[38;5;241m=\u001b[39m pad_sequences(encoded, maxlen \u001b[38;5;241m=\u001b[39m max_len)\n\u001b[0;32m---> 58\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(score \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m): \n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, 비속어 포함\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(score), \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:88\u001b[0m, in \u001b[0;36mdisable_multi_worker.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     86\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not supported in multi-worker mode.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     87\u001b[0m       method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1268\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1267\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 1268\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1269\u001b[0m   \u001b[38;5;66;03m# Catch OutOfRangeError for Datasets of unknown size.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m   \u001b[38;5;66;03m# This blocks until the batch has finished executing.\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m   \u001b[38;5;66;03m# TODO(b/150292341): Allow multiple async steps here.\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39minferred_steps:\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:580\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     xla_context\u001b[38;5;241m.\u001b[39mExit()\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracing_count():\n\u001b[1;32m    583\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_counter\u001b[38;5;241m.\u001b[39mcalled_without_tracing()\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:650\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    646\u001b[0m   canon_args, canon_kwds \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    647\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    648\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    649\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filtered_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanon_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanon_kwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(\u001b[38;5;241m*\u001b[39minner_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minner_kwds):\n\u001b[1;32m    653\u001b[0m   \u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1661\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_filtered_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs):\n\u001b[1;32m   1648\u001b[0m   \u001b[38;5;124;03m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m \n\u001b[1;32m   1650\u001b[0m \u001b[38;5;124;03m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03m    `args` and `kwargs`.\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1661\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseResourceVariable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1740\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1741\u001b[0m     pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:593\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    592\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    602\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    606\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[0,24] = 6814 is not in [0, 2908)\n\t [[node sequential_9/embedding_9/embedding_lookup (defined at tmp/ipykernel_31357/1176218959.py:58) ]] [Op:__inference_predict_function_1162]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_9/embedding_9/embedding_lookup:\n sequential_9/embedding_9/embedding_lookup/707 (defined at anaconda/envs/check_up_py38_PT_TF/lib/python3.8/contextlib.py:113)\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "slang_predict(\"근데 저게 이뻐보이나? 저 가격이면 쿠팡에서 두개 사고도 남음 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d3ffd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.99, 일반 댓글'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_predict(\"와 가격 미친거 아님? ㅠㅠ 남는게 있나요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3875bae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.82, 비속어 포함'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_predict(\"ㅋㅋㅋ 미친 저게 뭐야 ㅠ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5830fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.98, 일반 댓글'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_predict(\"파란색 존나 이쁘다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a00a530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.77, 비속어 포함'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slang_predict(\"존나 가 들어가면 욕설이라고 나와야하는데..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "206648d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[0,25] = 9820 is not in [0, 2908)\n\t [[node sequential_9/embedding_9/embedding_lookup (defined at tmp/ipykernel_31357/1176218959.py:58) ]] [Op:__inference_predict_function_1162]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_9/embedding_9/embedding_lookup:\n sequential_9/embedding_9/embedding_lookup/707 (defined at anaconda/envs/check_up_py38_PT_TF/lib/python3.8/contextlib.py:113)\n\nFunction call stack:\npredict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mslang_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m긴 댓글을 치면 비속어 분류 모델이 제대로 분류를 못하는것 같아요 ~\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mslang_predict\u001b[0;34m(new_sentence)\u001b[0m\n\u001b[1;32m     56\u001b[0m encoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([new_sentence])\n\u001b[1;32m     57\u001b[0m pad_new \u001b[38;5;241m=\u001b[39m pad_sequences(encoded, maxlen \u001b[38;5;241m=\u001b[39m max_len)\n\u001b[0;32m---> 58\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(score \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m): \n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, 비속어 포함\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(score), \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:88\u001b[0m, in \u001b[0;36mdisable_multi_worker.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     86\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not supported in multi-worker mode.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     87\u001b[0m       method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1268\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1267\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m-> 1268\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1269\u001b[0m   \u001b[38;5;66;03m# Catch OutOfRangeError for Datasets of unknown size.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m   \u001b[38;5;66;03m# This blocks until the batch has finished executing.\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m   \u001b[38;5;66;03m# TODO(b/150292341): Allow multiple async steps here.\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39minferred_steps:\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:580\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     xla_context\u001b[38;5;241m.\u001b[39mExit()\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 580\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracing_count():\n\u001b[1;32m    583\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_counter\u001b[38;5;241m.\u001b[39mcalled_without_tracing()\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:618\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    620\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    621\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2420\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2419\u001b[0m   graph_function, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filtered_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1661\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_filtered_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs):\n\u001b[1;32m   1648\u001b[0m   \u001b[38;5;124;03m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m \n\u001b[1;32m   1650\u001b[0m \u001b[38;5;124;03m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03m    `args` and `kwargs`.\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1661\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseResourceVariable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1740\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1741\u001b[0m     pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/function.py:593\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    592\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    602\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    605\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    606\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[0,25] = 9820 is not in [0, 2908)\n\t [[node sequential_9/embedding_9/embedding_lookup (defined at tmp/ipykernel_31357/1176218959.py:58) ]] [Op:__inference_predict_function_1162]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_9/embedding_9/embedding_lookup:\n sequential_9/embedding_9/embedding_lookup/707 (defined at anaconda/envs/check_up_py38_PT_TF/lib/python3.8/contextlib.py:113)\n\nFunction call stack:\npredict_function\n"
     ]
    }
   ],
   "source": [
    "slang_predict(\"긴 댓글을 치면 비속어 분류 모델이 제대로 분류를 못하는것 같아요 ~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb5135",
   "metadata": {},
   "source": [
    "## OOV 예제코드 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50130db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'love': 2, 'my': 3, 'i': 4, 'dog': 5, 'cat': 6, 'you': 7}\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# sentences = [\n",
    "#     'I love my dog',\n",
    "#     'I love my cat',\n",
    "#     'You love my dog!'\n",
    "# ]\n",
    "\n",
    "# tokenzier = Tokenizer(num_words = 100) #최대 빈도수 기준으로 100개까지만 사용\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "# word_index = tokenizer.word_index # 단어 : 토큰번호 딕셔너리 반환\n",
    "# print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebb5744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b33b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "[[], [], []]\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# sentence = [\n",
    "#     'I love my dog',\n",
    "#     'I love my cat',\n",
    "#     'You love my dog!',\n",
    "#     'Do you think my dog is amazing?'\n",
    "# ]\n",
    "\n",
    "# tokenizer = Tokenizer(num_words= 100) #얘를 바꾸는거!\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "# word_index = tokenzier.word_index\n",
    "\n",
    "# sequences = tokenzier.texts_to_sequences(sentences)\n",
    "\n",
    "# print(word_index)\n",
    "# print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c514086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edd53b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# padded = pad_sequences(sequences, padding = 'post', maxlen = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "757b9080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n",
      "\n",
      "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[0 0 0 0 0 5 1 3 2 4]\n",
      " [0 0 0 0 0 2 4 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# sentences = [\n",
    "#     'I love my dog',\n",
    "#     'I love my cat',\n",
    "#     'You love my dog!',\n",
    "#     'Do you think my dog is amazing?'\n",
    "# ]\n",
    "\n",
    "# tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "# sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# padded = pad_sequences(sequences, maxlen=5)\n",
    "# print(\"\\nWord Index = \" , word_index)\n",
    "# print(\"\\nSequences = \" , sequences)\n",
    "# print(\"\\nPadded Sequences:\")\n",
    "# print(padded)\n",
    "\n",
    "\n",
    "# # Try with words that the tokenizer wasn't fit to\n",
    "# test_data = [\n",
    "#     'i really love my dog',\n",
    "#     'my dog loves my manatee'\n",
    "# ]\n",
    "\n",
    "# test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "# print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "# padded = pad_sequences(test_seq, maxlen=10)\n",
    "# print(\"\\nPadded Test Sequence: \")\n",
    "# print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b7843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "903e581b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m datastore:\n\u001b[0;32m----> 9\u001b[0m     sentences\u001b[38;5;241m.\u001b[39mappend(\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheadline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     10\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_sarcastic\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m     urls\u001b[38;5;241m.\u001b[39mappend(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_link\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"./일베집합test3.json\", 'r') as f:\n",
    "#     datastore = json.load(f)\n",
    "\n",
    "\n",
    "# sentences = [] \n",
    "# for item in datastore:\n",
    "#     sentences.append(item['headline'])\n",
    "#     labels.append(item['is_sarcastic'])\n",
    "#     urls.append(item['article_link'])\n",
    "\n",
    "\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# print(len(word_index))\n",
    "# print(word_index)\n",
    "# sequences = tokenizer.texts_to_sequences(sentences)\n",
    "# padded = pad_sequences(sequences, padding='post')\n",
    "# print(padded[0])\n",
    "# print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc534706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('../일베집합_test4.json', encoding='utf-8') as json_file:\n",
    "#     vocab = json.load(json_file)\n",
    "#     tokenizer.word_index = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b044b6a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_train_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(vocab_size, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOOV\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#OOV 토큰\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(\u001b[43mclean_train_reviews\u001b[49m)\n\u001b[1;32m      3\u001b[0m train_input \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(clean_train_reviews)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_train_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "# tokenizer = Tokenizer(vocab_size, oov_token='OOV') #OOV 토큰\n",
    "# tokenizer.fit_on_texts(new_sentence)\n",
    "# train_input = tokenizer.texts_to_sequences(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9629a118",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_sorted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m word_to_index \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      2\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (word, frequency) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvocab_sorted\u001b[49m :\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frequency \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m : \u001b[38;5;66;03m# 빈도수가 작은 단어는 제외.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         i \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_sorted' is not defined"
     ]
    }
   ],
   "source": [
    "# word_to_index = {}\n",
    "# i = 0\n",
    "# for (word, frequency) in vocab_sorted :\n",
    "#     if frequency > 1 : # 빈도수가 작은 단어는 제외.\n",
    "#         i = i + 1\n",
    "#         word_to_index[word] = i\n",
    "\n",
    "# print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<OOV>\")\n",
    "# tokenizer.fit_on_texts(sentences)\n",
    "# word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8012f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_index = {}\n",
    "# word_to_index['OOV'] = len(word_to_index) + 1\n",
    "# print(word_to_index)\n",
    "# encoded_sentences = []\n",
    "\n",
    "# for sentence in preprocessed_sentences:\n",
    "#     encoded_sentence = []\n",
    "#     for word in sentence:\n",
    "#         try:\n",
    "#             # 단어 집합에 있는 단어라면 해당 단어의 정수를 리턴.\n",
    "#             encoded_sentence.append(word_to_index[word])\n",
    "#         except KeyError:\n",
    "#             # 만약 단어 집합에 없는 단어라면 'OOV'의 정수를 리턴.\n",
    "#             encoded_sentence.append(word_to_index['OOV'])\n",
    "#     encoded_sentences.append(encoded_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "check_up_py38_PT_TF",
   "language": "python",
   "name": "conda-env-check_up_py38_PT_TF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
