{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3dae558",
   "metadata": {},
   "source": [
    "## 일베 댓글+ 캐글 데이터로 fast-text 임베딩 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f537832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from gensim.models import FastText\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.python.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1523bc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>댓글</th>\n",
       "      <th>비속어여부</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>좌배 까는건</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>집에 롱 패딩만 세 개다 10년 더 입어야지</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>개소리야 니가 빨갱이를 옹호하고 드루킹을 짓이라고 말못해서 삐진거야 빨갱아</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>세탁이라고 봐도 된다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>애새끼가 초딩도 아니고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190576</th>\n",
       "      <td>드라마도 완전 재밌고 배우들도 멋있어서</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190577</th>\n",
       "      <td>원작을 읽을 때 이런 건 절대 영상화하기 힘들다고 생각했는데 벤휘쇼의 연기와 더불어...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190578</th>\n",
       "      <td>케석대 어깨 올라간거봐라</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190579</th>\n",
       "      <td>로버트다우니주니어를 좋아해서 봤는데너무재밌게 봤던영화생각없이 볼때 딱좋음</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190580</th>\n",
       "      <td>개지랄병 병신좌좀새끼</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190581 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       댓글  비속어여부\n",
       "0                                                  좌배 까는건      1\n",
       "1                                집에 롱 패딩만 세 개다 10년 더 입어야지      0\n",
       "2               개소리야 니가 빨갱이를 옹호하고 드루킹을 짓이라고 말못해서 삐진거야 빨갱아      1\n",
       "3                                             세탁이라고 봐도 된다      0\n",
       "4                                            애새끼가 초딩도 아니고      1\n",
       "...                                                   ...    ...\n",
       "190576                              드라마도 완전 재밌고 배우들도 멋있어서      0\n",
       "190577  원작을 읽을 때 이런 건 절대 영상화하기 힘들다고 생각했는데 벤휘쇼의 연기와 더불어...      0\n",
       "190578                                      케석대 어깨 올라간거봐라      1\n",
       "190579           로버트다우니주니어를 좋아해서 봤는데너무재밌게 봤던영화생각없이 볼때 딱좋음      0\n",
       "190580                                        개지랄병 병신좌좀새끼      1\n",
       "\n",
       "[190581 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./비속어댓글총합_특수문자,null값제거o.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5dbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d996782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 문장 내 한글자만 이루어진 댓글은 삭제\n",
    "\n",
    "# def onewordremove(sentence):\n",
    "#     sentence_list = []\n",
    "#     for index in range(len(sentence.split())):\n",
    "#         if len(sentence.split()[index]) > 1: #문자 1글자 초과일 때\n",
    "#             sentence_list.append(sentence.split()[index]) #문자 1글자로 이루어진 단어만 빼고 sentence_list에 추가\n",
    "#     sentence = \" \".join(sentence_list) #리스트의 내용을 하나의 문자열로 join\n",
    "#     return sentence\n",
    "\n",
    "# df['댓글'] = df['댓글'].apply(onewordremove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0221f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3cd862f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>댓글</th>\n",
       "      <th>비속어여부</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>좌배 까는건</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>집에 롱 패딩만 세 개다 10년 더 입어야지</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>개소리야 니가 빨갱이를 옹호하고 드루킹을 짓이라고 말못해서 삐진거야 빨갱아</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>세탁이라고 봐도 된다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>애새끼가 초딩도 아니고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>731부대의 후예라 그런지 가학적인 아이디어는 세계최고임 이래서 애교만 떨어도 돈 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>재앙이한건햇노</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>글쓴이 와꾸 승리에 비하면 방사능 피폭 원숭이 일듯</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>마 씨발련 아 몇평이고 맷개드갔노 니 대하이햄하고 해밨나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>은행에 대출 상담 받으러 가보면 직업의 귀천 바로 알려줌</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  댓글  비속어여부\n",
       "0                                             좌배 까는건      1\n",
       "1                           집에 롱 패딩만 세 개다 10년 더 입어야지      0\n",
       "2          개소리야 니가 빨갱이를 옹호하고 드루킹을 짓이라고 말못해서 삐진거야 빨갱아      1\n",
       "3                                        세탁이라고 봐도 된다      0\n",
       "4                                       애새끼가 초딩도 아니고      1\n",
       "5  731부대의 후예라 그런지 가학적인 아이디어는 세계최고임 이래서 애교만 떨어도 돈 ...      1\n",
       "6                                            재앙이한건햇노      1\n",
       "7                       글쓴이 와꾸 승리에 비하면 방사능 피폭 원숭이 일듯      1\n",
       "8                    마 씨발련 아 몇평이고 맷개드갔노 니 대하이햄하고 해밨나      1\n",
       "9                    은행에 대출 상담 받으러 가보면 직업의 귀천 바로 알려줌      0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4a43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ba0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dd1dc1d",
   "metadata": {},
   "source": [
    "# 자모 분류, fast text 는 git 참고 \n",
    "https://github.com/smothly/bad-word-detection/blob/master/FastTextVocab.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76299312",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CHOSUNGS = [u'ㄱ',u'ㄲ',u'ㄴ',u'ㄷ',u'ㄸ',u'ㄹ',u'ㅁ',u'ㅂ',u'ㅃ',u'ㅅ',u'ㅆ',u'ㅇ',u'ㅈ',u'ㅉ',u'ㅊ',u'ㅋ',u'ㅌ',u'ㅍ',u'ㅎ']\n",
    "JOONGSUNGS = [u'ㅏ',u'ㅐ',u'ㅑ',u'ㅒ',u'ㅓ',u'ㅔ',u'ㅕ',u'ㅖ',u'ㅗ',u'ㅘ',u'ㅙ',u'ㅚ',u'ㅛ',u'ㅜ',u'ㅝ',u'ㅞ',u'ㅟ',u'ㅠ',u'ㅡ',u'ㅢ',u'ㅣ']\n",
    "JONGSUNGS = [u'_',u'ㄱ',u'ㄲ',u'ㄳ',u'ㄴ',u'ㄵ',u'ㄶ',u'ㄷ',u'ㄹ',u'ㄺ',u'ㄻ',u'ㄼ',u'ㄽ',u'ㄾ',u'ㄿ',u'ㅀ',u'ㅁ',u'ㅂ',u'ㅄ',u'ㅅ',u'ㅆ',u'ㅇ',u'ㅈ',u'ㅊ',u'ㅋ',u'ㅌ',u'ㅍ',u'ㅎ']\n",
    "TOTAL = CHOSUNGS + JOONGSUNGS + JONGSUNGS\n",
    "\n",
    "# 자모분리\n",
    "def jamo_split(word, end_char=\"_\"):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for char in word:\n",
    "        \n",
    "        character_code = ord(char)\n",
    "        \n",
    "        if 0xD7A3 < character_code or character_code < 0xAC00:\n",
    "            result.append(char)\n",
    "            continue\n",
    "\n",
    "        chosung_index = int((((character_code - 0xAC00) / 28) / 21) % 19)\n",
    "        joongsung_index = int(((character_code - 0xAC00) / 28) % 21)\n",
    "        jongsung_index = int((character_code - 0xAC00) % 28)\n",
    "        \n",
    "        chosung = CHOSUNGS[chosung_index]\n",
    "        joongsung = JOONGSUNGS[joongsung_index]\n",
    "        jongsung = JONGSUNGS[jongsung_index]\n",
    "        \n",
    "        # 종성 범위 밖에 있는 것들은 end_char로 메꿔준다.\n",
    "        if jongsung_index == 0:\n",
    "            jongsung = end_char\n",
    "        \n",
    "        result.append(chosung)\n",
    "        result.append(joongsung)\n",
    "        result.append(jongsung)\n",
    "\n",
    "    return \"\".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "006f8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장을 jamo_split하여 저장\n",
    "#from JamoSplit import jamo_split, jamo_combine\n",
    "tmp.loc[:,'댓글'] = tmp.loc[:,'댓글'].apply(lambda x: jamo_split(x))\n",
    "tmp.loc[:,'댓글'] = tmp.loc[:,'댓글'].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cb71f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>댓글</th>\n",
       "      <th>비속어여부</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>[ㅇㅏ_ㅅㅡ_ㅍㅣ_ㄹㅣㄴ, ㄱㅜㄱㄴㅐ_ㅍㅏㄴㅁㅐ_, ㄱㅡㅁㅈㅣ_ㅇㅏ_ㄴㅣ_ㄴㅑ_]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18306</th>\n",
       "      <td>[ㄷㅡ_ㄹㅜ_ㅋㅣㅇㄸㅐ_ㅁㅜㄴㅇㅔ_, ㅈㅓㅂㅇㅓㅆㄷㅓㄴ, ㅇㅘ_ㅇㅜ_ㄷㅏ_ㅅㅣ_ㅎㅏ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>[ㅃㅏ_ㄱㅜ_ㄹㅣ_]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154745</th>\n",
       "      <td>[ㄱㅑ_ㄴㅡㄴ, ㄱㅏㄹㅋㅕ_, ㅈㅝ_ㄷㅗ_, ㅁㅗㅅ, ㅇㅏㄹㅇㅏ_, ㅊㅕ_ㅁㅓㄱㄴㅡ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106893</th>\n",
       "      <td>[ㄱㅗㄷ, ㅇㅣㅁㅈㅣㄴㅇㅙ_ㄹㅏㄴㄸㅐ_, ㅈㅗ_ㅅㅏㅇ, ㅈㅜㄱㅇㅓㅆㄷㅏ_ㄱㅗ_, ㅂ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161059</th>\n",
       "      <td>[ㄱㅡ_ㄴㅑㅇ, ㄱㅏ_ㅂㅕㅂㄱㅔ_, ㅂㅗ_ㅁㅕㄴ, ㄷㅚㅁ]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188003</th>\n",
       "      <td>[ㅇㅓ_ㅊㅏ_ㅍㅣ_, ㅈㅏ_ㅎㅏㄴㄷㅏㅇㄷㅗ_, ㅁㅣㄴㅈㅜ_ㄷㅏㅇ, 2ㅈㅜㅇㄷㅐ_ㅇㅣ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129382</th>\n",
       "      <td>[ㅇㅣ_ㅈㅓㅇㄷㅗ_ㅇㅢ_ㅍㅕㅇㅈㅓㅁㅇㅡㄴㅇㅏ_ㄴㅣㄴㄷㅔ_ㄴㅏ_ㄹㅡㅁㅈㅐ_ㅁㅣㅆㄴㅡㄴ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40506</th>\n",
       "      <td>[ㅍㅏ_ㄱㅕㄱㅈㅓㄱㅇㅣㄴ, ㅅㅗ_ㅈㅐ_ㅇㅢ_, ㅇㅣ_ㅇㅑ_ㄱㅣ_ㅇㅕㅆㄷㅏ_]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39233</th>\n",
       "      <td>[ㅇㅣ_ㄹㅓㅎㄱㅔ_, ㅂㅕㅇㅁㅏㅅㅅㅡ_ㄹㅓ_ㅇㅜㄴ, ㅍㅏㄴㅌㅏ_ㅈㅣ_, ㅅㅏ_ㄱㅡㄱ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64452</th>\n",
       "      <td>[ㅋㅗ_ㄷㅣㅇㅇㅣ_, ㅋㅓㅁㅍㅠ_ㅌㅓ_, ㅋㅗ_ㄷㅣㅇㅇㅏ_ㄴㅣㅁ]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94921</th>\n",
       "      <td>[ㅁㅕㅇㅎㅘ_ㅇㅔ_ㅅㅓ_, ㅇㅣ_ㄱㅓ_ㅂㅗ_ㄱㅗ_, ㅇㅜㅁ, ㅈㅣㄴㅉㅏ_ㄱㅏㅁㄷㅗㅇ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124646</th>\n",
       "      <td>[ㅅㅏㅇㅅㅏㅇㅎㅏ_ㄴㅣ_, ㅅㅓㅅㄷㅏ_]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25457</th>\n",
       "      <td>[ㅃㅓㄴㅎㅏㄹㅅㅜ_ㄷㅗ_, ㅇㅣㅆㄴㅡㄴ, ㅅㅗ_ㅈㅐ_ㄹㅡㄹ, ㅈㅐ_ㅁㅣㅆㄱㅔ_, ㅁ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10103</th>\n",
       "      <td>[40ㄷㅐ_ㅇㅢ_, ㅎㅕㄹㄱㅣ_ㅇㅘㅇㅅㅓㅇㅎㅏㅁㅇㅡㄹ, ㅂㅗ_ㅇㅕ_ㅈㅝㅆㄷㅓㄴ, ㄲ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       댓글  비속어여부\n",
       "34245       [ㅇㅏ_ㅅㅡ_ㅍㅣ_ㄹㅣㄴ, ㄱㅜㄱㄴㅐ_ㅍㅏㄴㅁㅐ_, ㄱㅡㅁㅈㅣ_ㅇㅏ_ㄴㅣ_ㄴㅑ_]      1\n",
       "18306   [ㄷㅡ_ㄹㅜ_ㅋㅣㅇㄸㅐ_ㅁㅜㄴㅇㅔ_, ㅈㅓㅂㅇㅓㅆㄷㅓㄴ, ㅇㅘ_ㅇㅜ_ㄷㅏ_ㅅㅣ_ㅎㅏ...      1\n",
       "5915                                          [ㅃㅏ_ㄱㅜ_ㄹㅣ_]      1\n",
       "154745  [ㄱㅑ_ㄴㅡㄴ, ㄱㅏㄹㅋㅕ_, ㅈㅝ_ㄷㅗ_, ㅁㅗㅅ, ㅇㅏㄹㅇㅏ_, ㅊㅕ_ㅁㅓㄱㄴㅡ...      1\n",
       "106893  [ㄱㅗㄷ, ㅇㅣㅁㅈㅣㄴㅇㅙ_ㄹㅏㄴㄸㅐ_, ㅈㅗ_ㅅㅏㅇ, ㅈㅜㄱㅇㅓㅆㄷㅏ_ㄱㅗ_, ㅂ...      1\n",
       "161059                   [ㄱㅡ_ㄴㅑㅇ, ㄱㅏ_ㅂㅕㅂㄱㅔ_, ㅂㅗ_ㅁㅕㄴ, ㄷㅚㅁ]      0\n",
       "188003  [ㅇㅓ_ㅊㅏ_ㅍㅣ_, ㅈㅏ_ㅎㅏㄴㄷㅏㅇㄷㅗ_, ㅁㅣㄴㅈㅜ_ㄷㅏㅇ, 2ㅈㅜㅇㄷㅐ_ㅇㅣ...      1\n",
       "129382  [ㅇㅣ_ㅈㅓㅇㄷㅗ_ㅇㅢ_ㅍㅕㅇㅈㅓㅁㅇㅡㄴㅇㅏ_ㄴㅣㄴㄷㅔ_ㄴㅏ_ㄹㅡㅁㅈㅐ_ㅁㅣㅆㄴㅡㄴ...      0\n",
       "40506          [ㅍㅏ_ㄱㅕㄱㅈㅓㄱㅇㅣㄴ, ㅅㅗ_ㅈㅐ_ㅇㅢ_, ㅇㅣ_ㅇㅑ_ㄱㅣ_ㅇㅕㅆㄷㅏ_]      0\n",
       "39233   [ㅇㅣ_ㄹㅓㅎㄱㅔ_, ㅂㅕㅇㅁㅏㅅㅅㅡ_ㄹㅓ_ㅇㅜㄴ, ㅍㅏㄴㅌㅏ_ㅈㅣ_, ㅅㅏ_ㄱㅡㄱ...      0\n",
       "64452                [ㅋㅗ_ㄷㅣㅇㅇㅣ_, ㅋㅓㅁㅍㅠ_ㅌㅓ_, ㅋㅗ_ㄷㅣㅇㅇㅏ_ㄴㅣㅁ]      1\n",
       "94921   [ㅁㅕㅇㅎㅘ_ㅇㅔ_ㅅㅓ_, ㅇㅣ_ㄱㅓ_ㅂㅗ_ㄱㅗ_, ㅇㅜㅁ, ㅈㅣㄴㅉㅏ_ㄱㅏㅁㄷㅗㅇ...      0\n",
       "124646                             [ㅅㅏㅇㅅㅏㅇㅎㅏ_ㄴㅣ_, ㅅㅓㅅㄷㅏ_]      1\n",
       "25457   [ㅃㅓㄴㅎㅏㄹㅅㅜ_ㄷㅗ_, ㅇㅣㅆㄴㅡㄴ, ㅅㅗ_ㅈㅐ_ㄹㅡㄹ, ㅈㅐ_ㅁㅣㅆㄱㅔ_, ㅁ...      0\n",
       "10103   [40ㄷㅐ_ㅇㅢ_, ㅎㅕㄹㄱㅣ_ㅇㅘㅇㅅㅓㅇㅎㅏㅁㅇㅡㄹ, ㅂㅗ_ㅇㅕ_ㅈㅝㅆㄷㅓㄴ, ㄲ...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c741143e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190581"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataframe -> list로 변환\n",
    "sentence_list = list(tmp['댓글'])\n",
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1531a05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ㅈㅘ_ㅂㅐ_', 'ㄲㅏ_ㄴㅡㄴㄱㅓㄴ'],\n",
       " ['ㅈㅣㅂㅇㅔ_',\n",
       "  'ㄹㅗㅇ',\n",
       "  'ㅍㅐ_ㄷㅣㅇㅁㅏㄴ',\n",
       "  'ㅅㅔ_',\n",
       "  'ㄱㅐ_ㄷㅏ_',\n",
       "  '10ㄴㅕㄴ',\n",
       "  'ㄷㅓ_',\n",
       "  'ㅇㅣㅂㅇㅓ_ㅇㅑ_ㅈㅣ_'],\n",
       " ['ㄱㅐ_ㅅㅗ_ㄹㅣ_ㅇㅑ_',\n",
       "  'ㄴㅣ_ㄱㅏ_',\n",
       "  'ㅃㅏㄹㄱㅐㅇㅇㅣ_ㄹㅡㄹ',\n",
       "  'ㅇㅗㅇㅎㅗ_ㅎㅏ_ㄱㅗ_',\n",
       "  'ㄷㅡ_ㄹㅜ_ㅋㅣㅇㅇㅡㄹ',\n",
       "  'ㅈㅣㅅㅇㅣ_ㄹㅏ_ㄱㅗ_',\n",
       "  'ㅁㅏㄹㅁㅗㅅㅎㅐ_ㅅㅓ_',\n",
       "  'ㅃㅣ_ㅈㅣㄴㄱㅓ_ㅇㅑ_',\n",
       "  'ㅃㅏㄹㄱㅐㅇㅇㅏ_'],\n",
       " ['ㅅㅔ_ㅌㅏㄱㅇㅣ_ㄹㅏ_ㄱㅗ_', 'ㅂㅘ_ㄷㅗ_', 'ㄷㅚㄴㄷㅏ_'],\n",
       " ['ㅇㅐ_ㅅㅐ_ㄲㅣ_ㄱㅏ_', 'ㅊㅗ_ㄷㅣㅇㄷㅗ_', 'ㅇㅏ_ㄴㅣ_ㄱㅗ_'],\n",
       " ['731ㅂㅜ_ㄷㅐ_ㅇㅢ_',\n",
       "  'ㅎㅜ_ㅇㅖ_ㄹㅏ_',\n",
       "  'ㄱㅡ_ㄹㅓㄴㅈㅣ_',\n",
       "  'ㄱㅏ_ㅎㅏㄱㅈㅓㄱㅇㅣㄴ',\n",
       "  'ㅇㅏ_ㅇㅣ_ㄷㅣ_ㅇㅓ_ㄴㅡㄴ',\n",
       "  'ㅅㅔ_ㄱㅖ_ㅊㅚ_ㄱㅗ_ㅇㅣㅁ',\n",
       "  'ㅇㅣ_ㄹㅐ_ㅅㅓ_',\n",
       "  'ㅇㅐ_ㄱㅛ_ㅁㅏㄴ',\n",
       "  'ㄸㅓㄹㅇㅓ_ㄷㅗ_',\n",
       "  'ㄷㅗㄴ',\n",
       "  'ㅂㅓㄹㄹㅣ_ㄴㅡㄴ',\n",
       "  'ㅎㅏㄴㄱㅜㄱㅇㅔ_',\n",
       "  'ㄱㅣ_ㄹㅡㄹ',\n",
       "  'ㅆㅓ_ㅅㅓ_',\n",
       "  'ㅈㅣㄴㅊㅜㄹㅎㅏ_ㄹㅕ_ㄱㅗ_',\n",
       "  'ㅎㅏ_ㅈㅣ_ㅈㅗ_ㅅㅔㄴㄴㅏㅁㅈㅏ_ㄷㅡㄹㅇㅡㄴ',\n",
       "  'ㄸㅗ_',\n",
       "  'ㅇㅣ_ㅃㅡㄴㅇㅕ_ㅈㅏ_ㅁㅏㄴ',\n",
       "  'ㅂㅗ_ㅁㅕㄴ',\n",
       "  'ㅅㅏ_ㅈㅗㄱㅇㅡㄹ',\n",
       "  'ㅁㅗㅅㅆㅡ_ㅁㅕ_',\n",
       "  'ㄱㅗㅇㅈㅜ_ㄷㅐ_ㅈㅓㅂㅎㅐ_ㅈㅜ_ㄴㅡㄴ',\n",
       "  'ㄴㅗㅁㄷㅡㄹㅇㅣ_ㄴㅣ_'],\n",
       " ['ㅈㅐ_ㅇㅏㅇㅇㅣ_ㅎㅏㄴㄱㅓㄴㅎㅐㅅㄴㅗ_'],\n",
       " ['ㄱㅡㄹㅆㅡㄴㅇㅣ_',\n",
       "  'ㅇㅘ_ㄲㅜ_',\n",
       "  'ㅅㅡㅇㄹㅣ_ㅇㅔ_',\n",
       "  'ㅂㅣ_ㅎㅏ_ㅁㅕㄴ',\n",
       "  'ㅂㅏㅇㅅㅏ_ㄴㅡㅇ',\n",
       "  'ㅍㅣ_ㅍㅗㄱ',\n",
       "  'ㅇㅝㄴㅅㅜㅇㅇㅣ_',\n",
       "  'ㅇㅣㄹㄷㅡㅅ'],\n",
       " ['ㅁㅏ_',\n",
       "  'ㅆㅣ_ㅂㅏㄹㄹㅕㄴ',\n",
       "  'ㅇㅏ_',\n",
       "  'ㅁㅕㅊㅍㅕㅇㅇㅣ_ㄱㅗ_',\n",
       "  'ㅁㅐㅅㄱㅐ_ㄷㅡ_ㄱㅏㅆㄴㅗ_',\n",
       "  'ㄴㅣ_',\n",
       "  'ㄷㅐ_ㅎㅏ_ㅇㅣ_ㅎㅐㅁㅎㅏ_ㄱㅗ_',\n",
       "  'ㅎㅐ_ㅂㅏㅆㄴㅏ_'],\n",
       " ['ㅇㅡㄴㅎㅐㅇㅇㅔ_',\n",
       "  'ㄷㅐ_ㅊㅜㄹ',\n",
       "  'ㅅㅏㅇㄷㅏㅁ',\n",
       "  'ㅂㅏㄷㅇㅡ_ㄹㅓ_',\n",
       "  'ㄱㅏ_ㅂㅗ_ㅁㅕㄴ',\n",
       "  'ㅈㅣㄱㅇㅓㅂㅇㅢ_',\n",
       "  'ㄱㅟ_ㅊㅓㄴ',\n",
       "  'ㅂㅏ_ㄹㅗ_',\n",
       "  'ㅇㅏㄹㄹㅕ_ㅈㅜㅁ']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fasttext 인풋 완성!\n",
    "sentence_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b30287",
   "metadata": {},
   "source": [
    "### FastText WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51a7e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext 적용\n",
    "from gensim.models import FastText\n",
    "# 임베딩 차원: 100\n",
    "# window size: 좌우 2단어 비속어는 좌우단어와 별로 연관이 없다고 판단...\n",
    "# min_count: 최소 3번 등장한 단어들\n",
    "# workers: -1 전부!!\n",
    "# sg: skipgram이 더 성능이 좋기 때문\n",
    "# min_n max_n : n-gram단위인데 한글자가 3글자라 최소 자모3개부터 최대 6개까지 ngram하기로 하였다. 1글자 ~ 2글자\n",
    "# iter: 반복횟수 10\n",
    "model = FastText(sentence_list, size=100, window=2, min_count=3, workers=4, sg=1, min_n=3, max_n=6, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67fa1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62105"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 총 62105개 단어로 vocab이 만들어졌다.\n",
    "# len(model.wv)\n",
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2383f874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2017ㄴㅕㄴ', 0.9939414262771606),\n",
       " ('2019ㄴㅕㄴ', 0.9929779171943665),\n",
       " ('2016ㄴㅕㄴ', 0.9876396656036377),\n",
       " ('2011ㄴㅕㄴ', 0.9789131283760071),\n",
       " ('18ㄴㅕㄴ', 0.9764711856842041),\n",
       " ('2012ㄴㅕㄴ', 0.9757429361343384),\n",
       " ('1988ㄴㅕㄴ', 0.9710288047790527),\n",
       " ('1982ㄴㅕㄴ', 0.9700126051902771),\n",
       " ('2002ㄴㅕㄴ', 0.9699717164039612),\n",
       " ('17ㄴㅕㄴ', 0.9687004089355469)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 연도 같은 경우는 연도와 비슷한 단어들이 나온다.\n",
    "model.wv.most_similar(jamo_split(\"2018년\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bad4edfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ㅈㅟ_ㅅㅐ_ㄲㅣ_', 0.9744609594345093),\n",
       " ('ㄱㅐ_ㅆㅣㅂㅅㅐ_ㄲㅣ_', 0.9732168912887573),\n",
       " ('ㅊㅣ_ㅁㅐ_ㅅㅐ_ㄲㅣ_', 0.9710700511932373),\n",
       " ('ㅇㅐ_ㅅㅐ_ㄲㅣ_', 0.9687486886978149),\n",
       " ('ㅉㅏㅇㄲㅐ_ㅅㅐ_ㄲㅣ_', 0.9680942893028259),\n",
       " ('ㅅㅣㅂㅅㅐ_ㄲㅣ_', 0.9667065739631653),\n",
       " ('ㄱㅐ_ㄷㅗㄱㅅㅐ_ㄲㅣ_', 0.9656611680984497),\n",
       " ('ㅊㅣ_ㅌㅏ_ㅅㅐ_ㄲㅣ_', 0.9641475677490234),\n",
       " ('ㅆㅣㅂㅅㅐ_ㄲㅣ_', 0.9631043076515198),\n",
       " ('ㅆㅣ_ㅂㅏㄹㄱㅐ_ㅅㅐ_ㄲㅣ_', 0.9628869891166687)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 욕설 같은 경우는 비슷한 형태의 욕설이 나온다.\n",
    "model.wv.most_similar(jamo_split(\"개새끼\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d657313b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ㅁㅝㄴㅅㅣ_ㅂㅏㄹ', 0.9585663080215454),\n",
       " ('ㅆㅣ_ㅂㅏㄹ', 0.9494709372520447),\n",
       " ('ㅅㅣ_ㅂㅏㅋ', 0.9484204053878784),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄴㅓㅁ', 0.9470741152763367),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄲㅓ_', 0.9322314262390137),\n",
       " ('ㅅㅣ_ㅂㅏㄹㅇㅘ_', 0.9310782551765442),\n",
       " ('ㅇㅘ_ㅅㅣ_ㅂㅏㄹ', 0.9289675951004028),\n",
       " ('ㅇㅘ_ㅆㅣ_ㅂㅏㄹ', 0.9286604523658752),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄴㅗㅁ', 0.9272249937057495),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄹㅕㄴ', 0.9139248132705688)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(jamo_split(\"시발\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78140479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ㅅㅣ_ㅂㅏㅋ', 0.9346566796302795),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄴㅓㅁ', 0.9328367710113525),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄹㅓㅁㅇㅏ_', 0.9319655895233154),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄹㅗㅁㅇㅏ_', 0.9312819838523865),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄲㅓ_', 0.9232179522514343),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄴㅗㅁ', 0.9213518500328064),\n",
       " ('ㅅㅣ_ㅂㅏㄹㅇㅏ_', 0.9189701676368713),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄹㅕㄴㅇㅏ_', 0.9157925844192505),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄴㅕㄴㅇㅏ_', 0.9128421545028687),\n",
       " ('ㅅㅣ_ㅂㅏㄹㄹㅕㄴ', 0.9093796610832214)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시발점을 쳤더니.. 아주 관련없는 단어들이 뜬다. 뭐지?\n",
    "model.wv.most_similar(jamo_split(\"시바\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1145ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ㅅㅐ_ㄲㅣㅋ', 0.9636362195014954),\n",
       " ('ㅈㅟ_ㅅㅐ_ㄲㅣ_', 0.9523922204971313),\n",
       " ('ㅅㅐ_ㄲㅣ_ㅇㅑㅋ', 0.9516310691833496),\n",
       " ('ㅇㅐ_ㅅㅐ_ㄲㅣ_', 0.9502369165420532),\n",
       " ('ㅅㅣㅂㅅㅐ_ㄲㅣ_', 0.9416285157203674),\n",
       " ('ㅆㅣㅂㅅㅐ_ㄲㅣ_', 0.9389625787734985),\n",
       " ('ㅁㅐ_ㄲㅣ_', 0.9385172128677368),\n",
       " ('ㅅㅐ_ㄲㅣㄴ', 0.9383877515792847),\n",
       " ('ㅇㅔㅁㅊㅏㅇㅅㅐ_ㄲㅣ_', 0.938178300857544),\n",
       " ('ㅆㅣㅂㅆㅐ_ㄲㅣ_', 0.9344974756240845)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(jamo_split(\"새끼\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fdf2c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ㅈㅗㄴㄴㅏ_ㅍㅐㅁ', 0.9659515619277954),\n",
       " ('ㅅㅗㄴㄴㅏ_', 0.9243826866149902),\n",
       " ('ㅈㅗ_ㅇㅗㄴㄴㅏ_', 0.9089362025260925),\n",
       " ('ㅈㅗㄴㄴㅏ_ㅎㅏㅁ', 0.8903926014900208),\n",
       " ('ㅈㅗㄴㄴㅏ_ㅇㅜㅅㅈㅏ_', 0.8892310857772827),\n",
       " ('ㅈㅛㄴㄴㅏ_', 0.8631578683853149),\n",
       " ('ㅈㅝㄴㄴㅏ_', 0.8607434034347534),\n",
       " ('ㅇㅛㄴㄴㅏ_', 0.8575743436813354),\n",
       " ('ㅈㅗㄴㄴㅏ_ㅁㅏㄶㄴㅔ_', 0.8511917591094971),\n",
       " ('ㅈㅗㄴㄴㅏ_ㅋㅡ_ㄴㅔ_', 0.8501119613647461)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(jamo_split(\"존나\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ad69ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ㅈㅣ_ㄹㅏㄹㅂㅕㅇ', 0.9671280980110168),\n",
       " ('ㅈㅣ_ㄹㅏㄹㅈㅗㅁ', 0.9590924978256226),\n",
       " ('ㅈㅣ_ㄹㅏㄹㅂㅏㄹㄱㅘㅇ', 0.9504424929618835),\n",
       " ('ㅈㅣ_ㄹㅏㄹㅁㅏ_', 0.9439529180526733),\n",
       " ('ㅈㅗㅈㅈㅣ_ㄹㅏㄹ', 0.9332311749458313),\n",
       " ('ㅈㅣ_ㄹㅏㄹㅇㅣㅁ', 0.9314278364181519),\n",
       " ('ㅈㅣ_ㄹㅏㄹㅎㅏㅁ', 0.928388774394989),\n",
       " ('ㅂㅕㄹㅈㅣ_ㄹㅏㄹ', 0.9173683524131775),\n",
       " ('ㅈㅣ_ㄹㅏㄹㅎㅏㄹ', 0.9025567770004272),\n",
       " ('ㄱㅐ_ㅈㅣ_ㄹㅏㄹ', 0.8838884234428406)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(jamo_split(\"지랄\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91bd21f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ㅆㅣㅂㄱㅓㄹㄹㅔ_', 0.9583634734153748),\n",
       " ('ㅎㅔ_ㅂㅓㄹㄹㅔ_', 0.9522801041603088),\n",
       " ('ㄱㅓㄹㄹㅔ_', 0.9474725723266602),\n",
       " ('ㅉㅏㅇㅋㅟ_ㅂㅓㄹㄹㅔ_', 0.9165828227996826),\n",
       " ('ㅊㅕ_ㅁㅓㄱㄴㅔ_', 0.8955965638160706),\n",
       " ('ㅂㅏ_ㅋㅟ_ㅂㅓㄹㄹㅔ_', 0.8930861353874207),\n",
       " ('ㅆㅣㅂㅅㅔ_', 0.8784026503562927),\n",
       " ('ㄱㅓㄹㄹㅕㅆㄴㅔ_', 0.8756577968597412),\n",
       " ('ㅁㅓㄱㄴㅔ_', 0.8675596117973328),\n",
       " ('ㄲㅏ_ㅍㅔ_', 0.862654447555542)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(jamo_split(\"벌레\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27af411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # embedding model 저장\n",
    "# model.save(\"./gensim_festtext.model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84b3af23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ㅁㅓㅇㅁㅓㅇㅇㅣ_', 0.9608525037765503),\n",
       " ('ㄲㅐㅇㄲㅐㅇㅇㅣ_', 0.9559065103530884),\n",
       " ('ㅈㅗㅈㄴㅑㅇㅇㅣ_', 0.9547886848449707),\n",
       " ('ㄸㅜㅇㄸㅐㅇㅇㅣ_', 0.9533778429031372),\n",
       " ('ㅇㅛㄱㅈㅐㅇㅇㅣ_', 0.9515029191970825),\n",
       " ('ㅇㅑㄱㅈㅐㅇㅇㅣ_', 0.9506490230560303),\n",
       " ('ㅇㅑㅇㅇㅣ_', 0.9498114585876465),\n",
       " ('ㅇㅡㄴㅎㅐㅇㅇㅣ_', 0.9482777118682861),\n",
       " ('ㅃㅐㅇㅃㅐㅇㅇㅣ_', 0.9477080702781677),\n",
       " ('ㄸㅗㅇㅇㅣ_', 0.9474722146987915)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.wv.most_similar(jamo_split(\"멍청이\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9dacb",
   "metadata": {},
   "source": [
    "### ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "831539ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext 모델 불러오기\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "embedding_model = FastText.load(\"./gensim_festtext.model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd157a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24033/1830680873.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  tmp['댓글'] = tmp['댓글'].apply(lambda x: [embedding_model[_] for _ in x])\n"
     ]
    }
   ],
   "source": [
    "# 각 단어를 벡터화 시켜주는 과정 3 x 100(embedding dimension) \n",
    "tmp['댓글'] = tmp['댓글'].apply(lambda x: [embedding_model[_] for _ in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd59b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화된 데이터로 저장\n",
    "tmp.to_json(\"./fast-text_vectorized_labeled_data_1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea2e17",
   "metadata": {},
   "source": [
    "## 패딩하기\n",
    "#### 문장마다 단어의 개수가 다르기 때문에 한 문장은 벡터의 개수가 전부 다르다. 벡터의 개수가 다를 경우 LSTM 모델 학습할 때 에러가 발생하기때문에 모든 데이터의 벡터의 개수를 맞춰 0으로 채워주는 패딩 작업 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155c8e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>댓글</th>\n",
       "      <th>비속어여부</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.7201249599, 0.301058948, -0.4723067284000...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.37243309620000004, 0.6573367715, 0.334138...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.9452651143, 1.1995326281, -0.6718460917, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-1.0153485537, 0.6960261464, 0.7171273828, -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.6345767379, 1.1871398687, -0.1858331263, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[0.12848372760000001, -0.1863826066, -0.03566...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[-0.6168811321000001, 0.6517000198, 0.0483778...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[-1.2506676911999999, 0.8527351022, -0.042807...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[0.24690280850000002, 0.9139938951000001, -0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[-0.40946820380000004, 0.4967239201, 0.268821...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  댓글  비속어여부\n",
       "0  [[-0.7201249599, 0.301058948, -0.4723067284000...      1\n",
       "1  [[-0.37243309620000004, 0.6573367715, 0.334138...      0\n",
       "2  [[-0.9452651143, 1.1995326281, -0.6718460917, ...      1\n",
       "3  [[-1.0153485537, 0.6960261464, 0.7171273828, -...      0\n",
       "4  [[-0.6345767379, 1.1871398687, -0.1858331263, ...      1\n",
       "5  [[0.12848372760000001, -0.1863826066, -0.03566...      1\n",
       "6  [[-0.6168811321000001, 0.6517000198, 0.0483778...      1\n",
       "7  [[-1.2506676911999999, 0.8527351022, -0.042807...      1\n",
       "8  [[0.24690280850000002, 0.9139938951000001, -0....      1\n",
       "9  [[-0.40946820380000004, 0.4967239201, 0.268821...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"./fast-text_vectorized_labeled_data_1.json\")\n",
    "data.columns = [\"댓글\", \"비속어여부\"]\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fefd69c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "8\n",
      "9\n",
      "3\n",
      "3\n",
      "23\n",
      "1\n",
      "8\n",
      "8\n",
      "9\n",
      "7\n",
      "13\n",
      "5\n",
      "7\n",
      "8\n",
      "4\n",
      "8\n",
      "19\n",
      "4\n",
      "2\n",
      "7\n",
      "20\n",
      "3\n",
      "8\n",
      "5\n",
      "4\n",
      "6\n",
      "3\n",
      "1\n",
      "15\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "8\n",
      "3\n",
      "4\n",
      "2\n",
      "44\n",
      "13\n",
      "5\n",
      "3\n",
      "2\n",
      "7\n",
      "16\n",
      "2\n",
      "7\n",
      "17\n",
      "2\n",
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 상위 50개 단어 개수 살펴보기\n",
    "for i in range(0,51):\n",
    "    print(len(data.iloc[i,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb55e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최대 단어길이 : 312\n",
      "문장의 평균 단어길이 : 8.244289829521305\n"
     ]
    }
   ],
   "source": [
    "print('문장의 최대 단어길이 :',max(len(review) for review in data['댓글']))\n",
    "print('문장의 평균 단어길이 :',sum(map(len, data['댓글']))/len(data['댓글']))\n",
    "\n",
    "# 한 문장의 최대 단어의 개수는 312개, 평균 단어의 개수는 8개 .. 이거맞아?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73627fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문장들 중 단어개수가 30 이하인 문장의 비율: 97.1733803474638\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    count = 0\n",
    "    for sentence in nested_list:\n",
    "        if(len(sentence) <= max_len):\n",
    "            count = count + 1\n",
    "    print('전체 문장들 중 단어개수가 %s 이하인 문장의 비율: %s'%(max_len, (count / len(nested_list))*100))\n",
    "    \n",
    "max_len = 30\n",
    "below_threshold_len(max_len, data['댓글'])\n",
    "\n",
    "# 97 % 가 30 단어 이하이기 때문에 최대단어수는 30으로 패딩해주는것이 좋을것같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01fbd1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152464 152464 38117 38117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(data['댓글'], data['비속어여부'] , test_size=0.2, random_state=0)\n",
    "print(len(train_x), len(train_y), len(test_x), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a320d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[-0.7201249599, 0.301058948, -0.4723067284000...\n",
       "1    [[-0.37243309620000004, 0.6573367715, 0.334138...\n",
       "2    [[-0.9452651143, 1.1995326281, -0.6718460917, ...\n",
       "3    [[-1.0153485537, 0.6960261464, 0.7171273828, -...\n",
       "4    [[-0.6345767379, 1.1871398687, -0.1858331263, ...\n",
       "5    [[0.12848372760000001, -0.1863826066, -0.03566...\n",
       "6    [[-0.6168811321000001, 0.6517000198, 0.0483778...\n",
       "7    [[-1.2506676911999999, 0.8527351022, -0.042807...\n",
       "8    [[0.24690280850000002, 0.9139938951000001, -0....\n",
       "9    [[-0.40946820380000004, 0.4967239201, 0.268821...\n",
       "Name: 댓글, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['댓글'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebde07dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46726     [[-0.3872094452, -0.16645319760000002, -0.1413...\n",
       "94165     [[-0.42452299590000003, 0.5644479394, -0.18073...\n",
       "136954    [[-1.252489686, -0.6533976197, 1.1044143438, 0...\n",
       "183719    [[-0.08469905700000001, 0.428263098, 0.5940571...\n",
       "133146    [[0.3221236169, 0.8550377488, -0.1347977668, 0...\n",
       "                                ...                        \n",
       "152315    [[-0.47730064390000004, -0.0412694402, -0.6019...\n",
       "176963    [[0.1267471761, -0.30235001440000003, 0.022919...\n",
       "117952    [[-0.6393437386, 0.0126002952, -0.3966500163, ...\n",
       "173685    [[-0.1671716422, -0.6286797523000001, -0.39739...\n",
       "43567     [[-0.1596149653, 0.7296592593, -0.467102944900...\n",
       "Name: 댓글, Length: 152464, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "532f3976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y array형식으로 변경\n",
    "y_train = np.array(train_y)\n",
    "y_test = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affde514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(train_x, maxlen=max_len)\n",
    "X_test = pad_sequences(test_x, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c51f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152464, 30, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a091a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38117, 30, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51192d39",
   "metadata": {},
   "source": [
    "### LSTM 학습하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f91c9772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 08:43:50.035205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-05-10 08:43:50.076834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0001:00:00.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2022-05-10 08:43:50.078838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-05-10 08:43:50.195343: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2022-05-10 08:43:50.248895: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-05-10 08:43:50.284837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-05-10 08:43:50.440911: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-05-10 08:43:50.446627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-05-10 08:43:50.446823: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2022-05-10 08:43:50.446836: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-10 08:43:50.447739: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-05-10 08:43:50.458268: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2445435000 Hz\n",
      "2022-05-10 08:43:50.458443: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0e08000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-05-10 08:43:50.458458: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-05-10 08:43:50.460448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-05-10 08:43:50.460460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      \n",
      "2022-05-10 08:43:50.732617: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1829568000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4758/4765 [============================>.] - ETA: 0s - loss: 0.3592 - accuracy: 0.8595"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 08:44:19.678602: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 457404000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4765/4765 [==============================] - 29s 6ms/step - loss: 0.3591 - accuracy: 0.8595 - val_loss: 0.2957 - val_accuracy: 0.8750\n",
      "Epoch 2/5\n",
      "4764/4765 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.8787"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 08:44:48.841744: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 457404000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4765/4765 [==============================] - 29s 6ms/step - loss: 0.2844 - accuracy: 0.8787 - val_loss: 0.2833 - val_accuracy: 0.8782\n",
      "Epoch 3/5\n",
      "4762/4765 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.8794"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 08:45:17.700566: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 457404000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4765/4765 [==============================] - 29s 6ms/step - loss: 0.2789 - accuracy: 0.8794 - val_loss: 0.2812 - val_accuracy: 0.8790\n",
      "Epoch 4/5\n",
      "4758/4765 [============================>.] - ETA: 0s - loss: 0.2777 - accuracy: 0.8801"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 08:45:46.484933: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 457404000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4765/4765 [==============================] - 29s 6ms/step - loss: 0.2777 - accuracy: 0.8801 - val_loss: 0.2807 - val_accuracy: 0.8785\n",
      "Epoch 5/5\n",
      "4765/4765 [==============================] - 29s 6ms/step - loss: 0.2773 - accuracy: 0.8798 - val_loss: 0.2804 - val_accuracy: 0.8787\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=1, input_shape=(30, 100)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "# mc = ModelCheckpoint('FT_best_model_v0.0.1.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5871a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.8787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28039073944091797, 0.8786892890930176]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "329d80ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.8787\n",
      "\n",
      " 테스트 정확도: 0.8787\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5083b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./fasttext_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1c856c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 09:01:21.980801: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./fasttext_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./fasttext_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bc90958",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('./fasttext_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617f4f1",
   "metadata": {},
   "source": [
    "## 예측하기 - 미완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "168111da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_result(jamo_split, loaded_model, lstm_model):\n",
    "    test_word = jamo_split(s)\n",
    "    test_word_split = test_word.split()\n",
    "    fast_vec = []\n",
    "    for index in range(sentence_number):\n",
    "        if index < len(test_word_split):\n",
    "            fast_vec.append(loaded_model[test_word_split[index]])\n",
    "        else:\n",
    "            fast_vec.append(np.array([0]*100))\n",
    "    fast_vec = np.array(fast_vec)\n",
    "    fast_vec=fast_vec.reshape(1, fast_vec.shape[0], fast_vec.shape[1])\n",
    "    # 학습 데이터와 마찬가지로 3차원으로 크기 조절\n",
    "    test_pre = lstm_model.predict_classes([fast_vec]) # 비속어 판별\n",
    "    if test_pre[0][0] == 0:\n",
    "        print(\"lstm 결과 : 비속어가 포함되어 있지 않습니다.\")\n",
    "    else:\n",
    "        print(\"lstm 결과 : 비속어가 포함되어 있습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "275c7101",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '_keras_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s_split \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39msplit():\n\u001b[1;32m      3\u001b[0m     test_word_run \u001b[38;5;241m=\u001b[39m s_split\n\u001b[0;32m----> 4\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjamo_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_word_run\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, word_temp \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:968\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m cast_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base_layer_utils\u001b[38;5;241m.\u001b[39mautocast_context_manager(\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype):\n\u001b[0;32m--> 968\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_mask_metadata(inputs, outputs, input_masks)\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:277\u001b[0m, in \u001b[0;36mSequential.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_graph_network(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 277\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSequential\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m outputs \u001b[38;5;241m=\u001b[39m inputs  \u001b[38;5;66;03m# handle the corner case where self.layers is empty\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    281\u001b[0m   \u001b[38;5;66;03m# During each iteration, `inputs` are the inputs to `layer`, and `outputs`\u001b[39;00m\n\u001b[1;32m    282\u001b[0m   \u001b[38;5;66;03m# are the outputs of `layer` applied to `inputs`. At the end of each\u001b[39;00m\n\u001b[1;32m    283\u001b[0m   \u001b[38;5;66;03m# iteration `inputs` is set to `outputs` to prepare for the next layer.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:717\u001b[0m, in \u001b[0;36mNetwork.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_graph_network:\n\u001b[1;32m    714\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen subclassing the `Model` class, you should\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    715\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m implement a `call` method.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_kwargs_to_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_layer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:832\u001b[0m, in \u001b[0;36mNetwork._run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    830\u001b[0m   masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_to_reference_inputs(mask)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_t, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, masks):\n\u001b[0;32m--> 832\u001b[0m   input_t\u001b[38;5;241m.\u001b[39m_keras_mask \u001b[38;5;241m=\u001b[39m mask\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Dictionary mapping reference tensors to computed tensors.\u001b[39;00m\n\u001b[1;32m    835\u001b[0m tensor_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_keras_mask'"
     ]
    }
   ],
   "source": [
    "s = '비속어가 포함된 문장'\n",
    "for s_split in s.split():\n",
    "    test_word_run = s_split\n",
    "    result = loaded_model(jamo_split(test_word_run))\n",
    "    count = 0\n",
    "    for _, word_temp in result:\n",
    "        for w in word_list:\n",
    "            if w in run_inverse(word_temp):\n",
    "                count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c985a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ce81c61",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_result(\u001b[38;5;241m*\u001b[39m\u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m가나다라마바사\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m가나다라마바사\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'test'"
     ]
    }
   ],
   "source": [
    "test_result(*loaded_model.test('가나다라마바사'))\n",
    "print(model.predict(\"가나다라마바사\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68c28f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = fasttext.train_supervised('cooking.stackexchange.txt', wordNgrams=2, epoch=25, lr=0.5)\n",
    "\n",
    "# model = fasttext.load_model(\"model_cooking.bin\")\n",
    "\n",
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "\n",
    "print_results(*model.test('cooking.stackexchange.txt'))\n",
    "\n",
    "print (model.predict(\"Which baking dish is best to bake a banana bread ?\"))\n",
    "# 3개의 카테고리 정보를 함께 출력\n",
    "# print (model.predict(\"Which baking dish is best to bake a banana bread ?\", k=3))\n",
    "\n",
    "model.save_model(\"model_cooking.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54b4ba01",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m시발\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m test_word_run \u001b[38;5;241m=\u001b[39m jamo_split(test_word)\n\u001b[0;32m----> 3\u001b[0m result\u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_word_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _,word_temp \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(run_inverse(word_temp))\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:88\u001b[0m, in \u001b[0;36mdisable_multi_worker.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     86\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not supported in multi-worker mode.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     87\u001b[0m       method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1240\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1237\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1239\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m   \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:1100\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m strategy \u001b[38;5;241m=\u001b[39m ds_context\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1115\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter\u001b[38;5;241m.\u001b[39mget_dataset()\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:650\u001b[0m, in \u001b[0;36mListsOfScalarsDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m   sample_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(sample_weights)\n\u001b[1;32m    647\u001b[0m sample_weight_modes \u001b[38;5;241m=\u001b[39m broadcast_sample_weight_modes(\n\u001b[1;32m    648\u001b[0m     sample_weights, sample_weight_modes)\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_adapter \u001b[38;5;241m=\u001b[39m \u001b[43mTensorLikeDataAdapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_modes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_modes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:275\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m (sample_weights, _, _) \u001b[38;5;241m=\u001b[39m training_utils\u001b[38;5;241m.\u001b[39mhandle_partial_sample_weights(\n\u001b[1;32m    271\u001b[0m     y, sample_weights, sample_weight_modes, check_all_flat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    273\u001b[0m inputs \u001b[38;5;241m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[0;32m--> 275\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_samples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    277\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData cardinality is ambiguous:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:275\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    270\u001b[0m (sample_weights, _, _) \u001b[38;5;241m=\u001b[39m training_utils\u001b[38;5;241m.\u001b[39mhandle_partial_sample_weights(\n\u001b[1;32m    271\u001b[0m     y, sample_weights, sample_weight_modes, check_all_flat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    273\u001b[0m inputs \u001b[38;5;241m=\u001b[39m pack_x_y_sample_weight(x, y, sample_weights)\n\u001b[0;32m--> 275\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mint\u001b[39m(\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(inputs))\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(num_samples) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    277\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData cardinality is ambiguous:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/check_up_py38_PT_TF/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:870\u001b[0m, in \u001b[0;36mTensorShape.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_behavior:\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims[key]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_word = '시발'\n",
    "test_word_run = jamo_split(test_word)\n",
    "result= loaded_model.predict(test_word_run)\n",
    "for _,word_temp in result:\n",
    "    print(run_inverse(word_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c10f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "check_up_py38_PT_TF",
   "language": "python",
   "name": "conda-env-check_up_py38_PT_TF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
